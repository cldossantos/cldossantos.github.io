{
  "hash": "7daffea5bdded63794ce83cca889c83a",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Processing an on-farm experiment\"\nauthor: \"Caio dos Santos\"\ndate: \"2025-03-13\"\ncategories: [pacu, yield monitor]\n---\n\n\n\n## Introduction\n\nIn a previous post, I have mentioned that *pacu* provided support for processing data coming from on-farm trials, as well as production fields. In this post, I want to explore and showcase this functionality a little more in depth. The idea is to provide an example of processing these data and make a few considerations that can greatly impact the end result.\n\nWe will look at simulated data from a small on-farm trial. This is made up data and I simulated a relationship between corn yield and nitrogen rate that followed a linear plateau relationship like the image below. The simulated relationship presented an intercept = 7000, slope = 30, and a breakpoint = 180. Let's process this data and see if we can recover this relationship. You can download the data [here](./raw-data.zip).\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-1-1.png){width=672}\n:::\n:::\n\n\n\n\n## Working with the data\n\nLoading the necessary packages\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(pacu)\nlibrary(sf)\nlibrary(nlraa)\nlibrary(nlme)\n```\n:::\n\n\n\n\n\n\nHere, we will import the data sets into R. We will use two data sets for this exercise. The first contains made up raw yield data that represents data coming from an on-farm trial. The second contains an example of nitrogen rates applied to each experimental unit.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nraw.yield <- read_sf('./raw-data/example-raw-data.shp',\n                     quiet = TRUE)\n\ntrial.design <- read_sf('./raw-data/example-trial-design.shp',\n                     quiet = TRUE)\n```\n:::\n\n\n\nLets take a look at how these two data sets line up in space. In this example, the points represent the yield monitor readings and the red rectangles represent the experimental units. We can see that, in this example, we had about 4 combine passes per experimental unit.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(st_geometry(raw.yield), pch = 1, cex = 0.3)\nplot(st_geometry(trial.design), border = 'red', add = TRUE)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n:::\n\n\n\nAnother piece of information that can be extracted from the *trial.design* data set is the amount of nitrogen applied to each experimental unit in this example.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(trial.design[\"nrate\"], main = 'Nitrogen rate (kg/ha)')\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n:::\n\n\n\nWe can take a look at the raw yield data to see the kind of data we are dealing with. We can see that there is a lot of variability in the yield data, with it ranging from $\\approx 200$ to $\\approx 12000$ kg/ha. Some of this variability comes from the treatment effect but there's a part of it that is just random variability. The challenge lies in identifying the treatment effect.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nboxplot(raw.yield$yld_kgh,\n        ylab = 'Yield (kg/ha)')\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n\n```{.r .cell-code}\nplot(raw.yield[\"yld_kgh\"],\n     pch = 16,\n     main = 'Yield (kg/ha)',\n     breaks = 'quantile',\n     reset = FALSE)\nplot(st_geometry(trial.design),\n     add = TRUE)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-7-2.png){width=672}\n:::\n:::\n\n\n\n## Cleaning based on standard deviation\n\nSomething that is common to do, is to use empirical rules to remove outliers and clean some of the noise inherent to these type of data, let's see the effect of that.\n\nA pretty common procedure is to clean anything outside of 2 or 3 standard deviations from the mean (you can read more about it [here](https://cldossantos.github.io/posts/2025-02-11_yield-map/)). Let us take a look at this procedure. I will isolate one experimental unit for us to take a closer look. The removed points are marked with a red \"X\". For this experimental unit, anything that was smaller tha 5622 or greater than 8139 kg/ha was removed.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## selecting only experimental unit 5\nto.keep <- as.numeric(st_intersects(raw.yield, trial.design[5, ]))\none.eu <- raw.yield[!is.na(to.keep), 'yld_kgh']\n\n## calculating the mean and sd\neu.mean <- mean(one.eu[['yld_kgh']])\neu.sd <- sd(one.eu[['yld_kgh']])\n\n## defining upper and lower boundaries and identifying which\n## data points to remove\nupper.boundary <- eu.mean + 2 * eu.sd\nlower.boundary <- eu.mean - 2 * eu.sd\nto.remove <- (one.eu$yld_kgh < lower.boundary | \n                one.eu$yld_kgh > upper.boundary)\n\n## showing which points were removed\nplot(one.eu, reset = FALSE,\n     main = 'Yield (kg/ha)',\n     pch = 16)\nplot(st_geometry(one.eu)[to.remove],\n     cex = 2,\n     col = 'red',\n     pch = 'x', \n     add = TRUE)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n:::\n\n\n\nWe can now apply this same methodology to all experimental units and see that, in total, we will remove 60 points from this data set.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nraw.yield$eu <- as.numeric(st_intersects(raw.yield, trial.design))\nraw.yield <- raw.yield[order(raw.yield$eu), ]\nto.remove <- ave(raw.yield$yld_kgh, \n                 raw.yield$eu,\n                 FUN = function(x){\n                   eu.mean <- mean(x)\n                   eu.sd <- sd(x)\n                   upper.boundary <- eu.mean + 2 * eu.sd\n                   lower.boundary <- eu.mean - 2 * eu.sd\n                    (x < lower.boundary | x > upper.boundary)\n                 })\nto.remove <- as.logical(to.remove)\n\nplot(raw.yield['yld_kgh'], \n     main = 'Yield (kg/ha)',\n     reset = FALSE,\n     pch = 16)\nplot(st_geometry(raw.yield)[to.remove],\n     col = 'red',\n     pch = 'x', \n     add = TRUE)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-9-1.png){width=672}\n:::\n:::\n\n\n\nNow, let us remove these data points, and average the yield observations within each experimental unit. We can see that our estimates of the model parameters are not quite the same ones that we simulated. In addition, the variance of the parameter estimates is pretty large. This means that we are not very certain of these values.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nyield.filtered.sd <- raw.yield[!to.remove, ]\nmean.yield.sd <- aggregate(yield.filtered.sd['yld_kgh'],\n                           trial.design,\n                           FUN = mean)\nmean.yield.sd <- st_join(mean.yield.sd, trial.design, join = st_equals)\n\n\nfit1 <- nls(yld_kgh ~ SSlinp(nrate, a, b, xs),\n            data = mean.yield.sd)\n# estimates\ncoef(fit1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n         a          b         xs \n6814.09790   28.01518  168.74310 \n```\n\n\n:::\n\n```{.r .cell-code}\n# variances\ndiag(vcov(fit1))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n           a            b           xs \n28833.979455     3.075624    71.175162 \n```\n\n\n:::\n\n```{.r .cell-code}\n## Visualizing the data\n\npreds <- data.frame(nrate = 0:300)\npreds <- cbind(preds,  predict_nls(fit1,\n                                   interval = 'conf',\n                                   newdata = preds))\n\nplot(mean.yield.sd$nrate, mean.yield.sd$yld_kgh, \n     xlab = 'Nitrogen rate (kg/ha)',\n     ylab = 'Yield (kg/ha)')\npolygon(x = c(preds$nrate, rev(preds$nrate)),\n        y = c(preds$Q2.5, rev(preds$Q97.5) ),\n        border = 'steelblue', lty = 2) \nlines(preds$nrate, preds$Estimate, \n      col = 'blue')\nlegend('bottomright',\n       lty = 1:2,\n       legend = c('Estimate', 'CI'),\n       col = c('blue', 'steelblue'))\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-10-1.png){width=672}\n:::\n:::\n\n\n\n## Adding a buffer to the experimental units\n\nAnother empirical cleaning method that is commonly used is to add a buffer to the experimental units. This in meant to remove some of the border effect or the transition between experimental units. However, adding a buffer needs to be done with caution. You want your buffer to be big enough to remove some of these potentially problematic observations, but you do not want your buffer to remove so many points that you will end up estimating the mean with few points. This would increase the uncertainty of your estimate. Let's take a look at different buffer sizes:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbuffers <- c(1, 2, 5, 10)\ncols <- hcl.colors(4, palette = 'Temps')\nplot(st_geometry(raw.yield), cex = 0.5)\nfor (i in 1:length(buffers)){\n  buffered.exp.units <- st_as_sf(st_buffer(trial.design, -buffers[i]))\n  plot(st_geometry(buffered.exp.units), border = cols[i], add = TRUE)\n}\nlegend('topleft', \n       fill = cols,\n       legend = buffers,\n       title = 'Buffer size (m)')\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-11-1.png){width=672}\n:::\n:::\n\n\n\nIt seems that a buffer size of about 5 meters is what we want for this example. Let's go with that and see which observations will be removed from the data set:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbuffered.exp.units <- st_as_sf(st_buffer(trial.design, -5))\nraw.yield$eu <- as.numeric(st_intersects(raw.yield, buffered.exp.units))\nraw.yield <- raw.yield[order(raw.yield$eu), ]\nto.remove <- ave(raw.yield$yld_kgh, \n                 raw.yield$eu,\n                 FUN = function(x){\n                   eu.mean <- mean(x)\n                   eu.sd <- sd(x)\n                   upper.boundary <- eu.mean + 2 * eu.sd\n                   lower.boundary <- eu.mean - 2 * eu.sd\n                    (x < lower.boundary | x > upper.boundary)\n                 })\n\nto.remove <- as.logical(to.remove)\n\nplot(raw.yield['yld_kgh'], \n     main = 'Yield (kg/ha)',\n     reset = FALSE,\n     pch = 16)\nplot(st_geometry(raw.yield)[to.remove],\n     col = 'red',\n     pch = 'x', \n     add = TRUE)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-12-1.png){width=672}\n:::\n:::\n\n\n\nNow, let us remove these data points, and average the yield observations within each experimental unit. We can see that our estimates of the model parameters are a little closer to the true values but still not quite there. Also, the variance has decreased quite a bit but let's see if we can increase the precision of our estimates!\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nyield.filtered.buffer <- raw.yield[!to.remove, ]\nmean.yield.buffer <- aggregate(yield.filtered.buffer['yld_kgh'],\n                           trial.design,\n                           FUN = mean)\nmean.yield.buffer <- st_join(mean.yield.buffer, trial.design, join = st_equals)\n\n\nfit2 <- nls(yld_kgh ~ SSlinp(nrate, a, b, xs),\n            data = mean.yield.buffer)\n## estimates\ncoef(fit2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n        a         b        xs \n6659.5802   30.3704  172.9306 \n```\n\n\n:::\n\n```{.r .cell-code}\n## variances\ndiag(vcov(fit2))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n          a           b          xs \n6649.190326    0.709247   14.583390 \n```\n\n\n:::\n\n```{.r .cell-code}\n## Visualizing the data\n\npreds <- data.frame(nrate = 0:300)\npreds <- cbind(preds,  predict_nls(fit2,\n                                   interval = 'conf',\n                                   newdata = preds))\n\nplot(mean.yield.buffer$nrate, \n     mean.yield.buffer$yld_kgh, \n     xlab = 'Nitrogen rate (kg/ha)',\n     ylab = 'Yield (kg/ha)')\npolygon(x = c(preds$nrate, rev(preds$nrate)),\n        y = c(preds$Q2.5, rev(preds$Q97.5) ),\n        border = 'steelblue', lty = 2) \nlines(preds$nrate, preds$Estimate, \n      col = 'blue')\n\nlegend('bottomright',\n       lty = 1:2,\n       legend = c('Estimate', 'CI'),\n       col = c('blue', 'steelblue'))\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-13-1.png){width=672}\n:::\n:::\n\n\n\nWe get a little closer to the true value with every layer of processing that we include in this exercise. However, these layers are empirical and somewhat arbitrary. By adding a buffer zone, we are looking to remove points that have an influence from the adjacent experimental units. This can be done using the *ritas* algorithm within *pacu*.\n\n## Using pa_yield\n\nThe pa_yield function has built-in capabilities to automate these processes without the need for these empirical rules. For instance, by setting the argument *remove.crossed.polygons* to *TRUE*, we remove data that could be influenced by adjacent experimental units. You can read more about the *ritas* algorithm [here](https://doi.org/10.1016/j.softx.2024.101971).\n\nSo we can visualize these steps, I will set the option *steps* to *TRUE*. This is a new addition to the package and is not yet available on CRAN. Please install the package from [GitHub](https://github.com/cldossantos/pacu) to be able to use this.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nyld3 <- pa_yield(raw.yield,\n                 data.columns = c(mass = 'mass_g',\n                                  interval = 'intrvl_',\n                                  distance = 'dstnc_f',\n                                  width = 'swth_ft',\n                                  angle = 'angl_dg',\n                                  moisture = 'moistur'),\n                 grid = trial.design,\n                 steps = TRUE,\n                 algorithm = 'ritas',\n                 unit.system = 'metric',\n                 verbose = FALSE,\n                 remove.crossed.polygons = TRUE,\n                 cores = 6)\n\nyld4 <- st_join(yld3$yield, trial.design, join = st_equals)\nyld4$yield_kgh <- yld4$yield * 1000 ## converting from t/ha\n                                    ## to kg/ha    \nfit3 <- nls(yield_kgh ~ SSlinp(nrate, a, b, xs),\n            data = yld4)\n\n## estimates\ncoef(fit3)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n         a          b         xs \n6997.17524   30.40087  176.09137 \n```\n\n\n:::\n\n```{.r .cell-code}\n## variances\ndiag(vcov(fit3))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n           a            b           xs \n1401.4136937    0.1494841    3.1692490 \n```\n\n\n:::\n\n```{.r .cell-code}\n## Visualizing the data\n\npreds <- data.frame(nrate = 0:300)\npreds <- cbind(preds,  predict_nls(fit3,\n                                   interval = 'conf',\n                                   newdata = preds))\n\nplot(yld4$nrate, \n     yld4$yield_kgh, \n     xlab = 'Nitrogen rate (kg/ha)',\n     ylab = 'Yield (kg/ha)')\npolygon(x = c(preds$nrate, rev(preds$nrate)),\n        y = c(preds$Q2.5, rev(preds$Q97.5) ),\n        border = 'steelblue', lty = 2) \nlines(preds$nrate, preds$Estimate, \n      col = 'blue')\n\n\nlegend('bottomright',\n       lty = 1:2,\n       legend = c('Estimate', 'CI'),\n       col = c('blue', 'steelblue'))\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-14-1.png){width=672}\n:::\n:::\n\n\n\nWe can see that the *pa\\_yield()* function was able to produce more precise estimates of the model parameters. In part, this is due to the *ritas* algorithm recreating the destructive harvest process and being able to track which harvest polygons have crossed between different experimental units. We can see that in the plot below that demonstrates which polygons were removed:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwith(yld3$steps, {\n  plot(adjusted.polygons, border = 'red')\n  plot(cleaned.polygons, add = TRUE)\n  plot(grid, add = TRUE, border = 'blue')\n  legend('topleft',\n         fill = rep(NA, 3),\n         border = c('black', 'red', 'blue'),\n         legend = c('harvest polygons', \n                    'removed polygons',\n                    'experimental units'))\n})\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-15-1.png){width=672}\n:::\n:::\n\n\n\n\n\n## Conclusion\n\nIn this exercise, we have seen how different decisions about data processing can affect the estimates we get from the yield monitor data. We have also seen how we can use the *pa_yield()* function to process the yield data coming from agronomic trials. The differences in the precision of the estimates are really important when we want to estimate confidence intervals or investigate whether a certain covariate has an effect. For instance, let's say that, in addition to nitrogen rates, we also had nitrogen source. More precise estimates would increase the power of our analysis to find differences.\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}