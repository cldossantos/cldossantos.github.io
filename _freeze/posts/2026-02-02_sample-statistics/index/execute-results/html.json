{
  "hash": "42ee5aff959da4015638e503dee7bc33",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Sample Statistics\"\nsubtitle: \"AGRON 5130 - Module 3\"\nauthor: \"Caio dos Santos\"\ndate: \"2026-02-02\"\noutput:\n  html_document:\n    toc: true\n    toc_float: true\n    theme: flatly\n    highlight: tango\n    smooth: false\n---\n\n\n\n\n\n# Sampling Distributions: Linking Samples to Populations\n\n\nIn the last two modules, we focused on populations\nand on how to summarize them when we are able to\nmeasure every individual. In that context,\nquantities like the mean and the standard\ndeviation describe the entire population, because\nthey are calculated using all available\nobservations.\n\nIn agricultural science this is almost never\npossible, because the volume of fieldwork required\nto measure every single individual would simply be\nprohibitive to the work.  If you have done field work\nbefore, can you imagine having to measure every\nsingle plant within an experimental unit? Instead,\nwe rely on **samples**, right? This means that we\ncollect from a subset of individuals and use those\nto learn about the population as a whole. \n\nWhen we work with samples, instead of the whole\npopulation, we no longer can quantify the\npopulation mean ($\\mu$) and standard deviation\n($\\sigma$) with certainty. Instead, we need to\nestimate these parameters from the information we\nretrieved from the samples. Now, this raises a\ncouple questions, right? For example:\n\n\n- Was the sample representative of the population?\n- If we took a different random sample, would we get\nsimilar estimates?\n- And how far might a sample mean\nbe from the true population mean?\n- Should we have taken more samples perhaps?\n\nIn other words, statistics computed from samples\ninvolve increased uncertainty. We rarely have\ncomplete confidence that a single sample mean\nperfectly represents the population mean.\n\nThis is where sampling distributions come in.\nThrough replicated samples and examining how their\nmeans vary, we can describe the distribution of\nsample means around the population mean. This idea\nsits at the core of statistical inference: we use\nvariability among samples to quantify how accurate\nour estimates are and how much uncertainty we\nshould expect when making statements about a\npopulation.\n\n\n# Case study before we look at theory\n\nThere's quite a bit of statistical theory that\ninforms how we can use the sampled data to make\nstatements about the population. However, I think\na motivating example before we introduce that\ntheory can be beneficial here.\n\n\nLet's take a look at the same soybean yield data\nthat we looked at before. Just to remind you, take\na look at the yield map:\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-1-1.png){width=672}\n:::\n:::\n\n\nLet's pretend that you did not have this yield map\nand you wanted to sample this field to tell your\nboss (or your neighbor) what the average yield in\nthis field was.\n\n## One sample \n\nLet's say that you were short on resources and\ncould only sample this field once. How variable\ncould your sample mean be?\n\nTo establish an easy way to compare the next\nsteps, let's draw a red line representing the\n**population** mean ($\\mu$).  Then, we can\nrepresent the **sample** mean ($\\bar{x}$) in blue,\nand the sampled points as black dots.\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n:::\n\n\nBy chance, with one sample, we would end up with a sample average of 71.74 bu/ac. But, this is one time, right? Let's say I had gone to that field in a different day and had sampled a different location. As we can see from the graph below, with one sample, I could have gotten an average of 89.95 bu/ac. This illustrates that when we have low sample sizes, there's a lot of variability in our measurements. This means you're getting two very different answers simply by because of the location within the field that you sampled.\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n:::\n\n\n## Ten samples\n\nOk, so let's say you got some more resources to\nincrease your sample size. Let's see what happens\nwith ten samples.\n\nWe can see that with 10 samples, we were able to get closer to the true mean on this iteration. Even though we had samples above and below $\\mu$, on average, we are close. \n\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n:::\n\n\nIf we had sampled ten times in other parts of the\nfield, the result would be the graph below. We are\nstill close to $\\mu$ again. This shows that\ngreater sample sizes will decrease the variability\nin the sample mean.\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n:::\n\n\n## Visualizing the relationship between sample size and variability\n\nSo we can see how much the sample size can\ninfluence the variability of our sample mean,\nlet's do an exercise together. I will repeat the\nsame procedure as before 10 times (10 iterations)\nand plot all of them in the same graph. The number on\neach panel represents the sample size.\n\nIn the graph below, you can see on the y-axis the\niteration number and on the x-axis the yield value\nin bu/ac. Just like before, the red line still\nrepresents the **population** mean. The multiple\nblue lines now represent the **sample** mean for\neach iteration. As you can see, as the sample size\nincreases, the  **sample** mean lines cluster\naround the **population** mean. This shows this\ndecrease in variability of the **sample** mean as\na function of the increase in sample size.\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n:::\n\n\n\n# Theory\n\n## Standard error of the mean \n\nThis decrease in variability as a function of an\nincrease in sample size is well represented by a\nstatistic called the standard error of the mean,\nwhich can be calculated as follows:\n\n$$\\sigma_{\\bar{x}} = \\frac{\\sigma}{\\sqrt{n}}$$\n\n\n## Central Limit Theorem\n\nOne of the most important theorems in statistics\nis the Central Limit Theorem (CLT), which states the\nfollowing:\n\n*The distribution of the sample mean can be approximated as normal with mean $=\\mu$ and variance $=\\frac{\\sigma^2}{n}$ when $n$ is large*\n\nIn practice, this approximation is used when $n\n\\geq 30$ regardless of the distribution of the\npopulation sampled. For smaller $n$ this\napproximation can be questionable.\n\nYou may recognize that the variance of the\nsampling distribution is the square of the\nstandard error of the mean.\n\n\n### Illustrating this principle\n\nI thought you might appreciate seeing the Central\nLimit Theorem in action. In the yield monitor data\nexample, the yield data was approximately normally\ndistributed already, so it made sense that the\nsampling means distribution would also be\napproximately normal.\n\n\nWe can take a look at the CLT in action in a case in which the population distribution is not approximately normal. I will not go much in detail about the distribution itself, as it serves only to demonstrate the CLT. \n\nWe can see that the data below does not resemble a symmetrical bell-shaped population. It's heavily skewed towards lower values. In this illustrative case, the **population** mean is 60.\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n:::\n\n\nBelow, is some code that will take 30 samples ($n=30$) from this population 10,000 times so that we can create a distribution. I am including it as an example but please do not focus on it too much. The important thing is that it represents us sampling from this clearly not normally distributed population many times.Then, the code plots a histogram of the mean. Can you see how, even though the population mean is not, the sampling mean is approximately normal?\n\n\n::: {.cell}\n\n```{.r .cell-code}\nres <- c()\nfor (i in 1:1e4){\nsample <- rgamma(30, shape = 2, scale = 30) \nxbar <- mean(sample)\nres <- c(res, xbar)\n}\nhist(res, \n     main = 'Histogram of the sample mean',\n     xlab = 'Sample mean')\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n:::\n\n\n\n\n# The $t$ Distribution\n\nWhen talking about the CLT, we said that the\nsample mean distribution approximates to a normal\ndistribution with variance $\\frac{\\sigma^2}{n}$.\nWhile this is true, it makes this concept hard to\napply in practice, as for most applications the\npopulation standard deviation ($\\sigma$) is\n**unknown**. Therefore, we need to replace it for\nan estimate, which is usually the sample standard\ndeviation ($S$). Thus, the theory leads to another\ndistribution, called the $t$ distribution.\n\nDo you remember in the previous module that I\ntalked about the standard normal distribution\n($Z$)? The $t$ distribution will have a similar\nshape, but it also accounts for the fact that we\nare adding uncertainty because we do not know the\n**population* standard deviation ($\\sigma$), and\nwe are estimating it with the sample standard\nerror ($S$). Therefore, the $T$ statistic can be\ncalculated as follows:\n\n$$T = \\frac{\\bar{X} - \\mu}{\\frac{S}{\\sqrt{n}}}$$\n\n## Particularities of the $t$ distribution\n\nIn practice, this estimation of the **population**\nstandard deviation by the **sample** standard\nerror has an important implication for the $t$\ndistribution. The distribution changes shape\naccording to the number of samples you have\ncollected. This is where we introduce the concept\nof **degrees of freedom**, which can is calculated\nas $n-1$. \n\n### Quick look into degrees of freedom\n\nThere's a more formal definition of **degrees of\nfreedom** that involves linear algebra but, for\nthis class, we can think of **degrees of freedom**\nas how much independent information we actually\nhave left once we estimate something from the\ndata.\n\nWhen we collect a sample and calculate a sample\nmean, that mean is fixed. Once the mean is fixed,\nnot all observations are free to vary\nindependently anymore. The last observation is\nconstrained by the ones that came before it.\n\n### Relationship between sample size and the $t$ distribution \n\nLet's take a look at the $t$\ndistribution in comparison to the standard normal\ndistribution for different sample sizes, and then\nwe can think about the effect of sample size.\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-9-1.png){width=672}\n:::\n:::\n\n\nAn important characteristic of the $t$\ndistribution is that, as the sample size\nincreases, the more it approximates to the normal\ndistribution. However, as we can see in the red\nline above, when sample sizes are low, more\nextreme values, for example $|T| \\geq 2$ are more\ncommon. This means that, when using the $t$\ndistribution to find differences among treatments,\nfor example, if the sample size is low, you might\nfail to detect those differences simply because\nmore extreme values are common.\n\n\n### Curious fact\n\nYou might be familiar with the $t$ distribution's\nstory. It was developed by W. S. Gosset, while\nworking at Guiness, which did not allow employees\nto publish their work. The work was then published\nunder the name of *student*, and the distribution\nis sometimes also called *Student t distribution*.\n\n\n\n## The $t$ distribution in practice\n\nOkay, by this point you might be wondering exactly\nhow we will use this concept in practice. I wanted\nto give you an example to help you visualize how\nthis can be useful.\n\n### Case study\n\nLet's say you went to a corn field and collected\nyield samples randomly at 16 locations. The yield\naveraged 13.8 t/ha with a standard deviation of\n2.1 t/ha. Your boss had told you that if you could\nfind statistical evidence that the yield was above\n12 t/ha, you would get a bonus for being such a\ngood agronomist.\n\nLet's put the $t$ distribution in practice and get\nthat bonus! \n\n$$T = \\frac{\\bar{X} - \\mu}{\\frac{S}{\\sqrt{n}}} =\n\\frac{13.8 - 12.0}{\\frac{2.1}{\\sqrt{16}}} = 3.42$$\n\nGreat! We got a t-value that we can use\nto compare to the probability. Let's use the\n`fastGraph` library again and check what is the\nprobability that we get a value t-value outside of\n3.42.\n\nWe can see below that the probability is pretty\nnegligible. Can you even see the red shaded area?\nTherefore, you could tell your boss to\nwrite you a big check!\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(fastGraph)\nshadeDist(xshade = c(-3.42, 3.42),\n          ddist = 'dt',\n          parm1 = 15) ## degrees of freedom: n - 1\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-10-1.png){width=672}\n:::\n:::\n\n\n\n\n## Confidence intervals\n\nWe can flip the relationships between $t$ and\nprobability to talk about confidence intervals.\nLet's say that instead of testing whether a\nt-value provides evidence that your mean is\ndifferent from a given value, you want to state an\ninterval for which the **population** mean is likely\nto fall within.\n\nDoing some math, we can say that the confidence\ninterval (CI) can be calculated using the formula\nbelow, in which the $t$ value corresponds to the\nsignificance level.\n\n$$CI = \\bar{x} \\pm t \\times \\frac{S}{\\sqrt{n}}$$\n\n\nLet's compute the CI for the previous example and\nlet's use n = 16.\n\nIn the past, we used t-value tables to guide us in\nthese tests. Have any of you ever seen these? I\nused them in undergrad.\n\n![](./t-table.png)\n\nLuckily, in R, we can retrieve the same\ninformation using the `qt()` function, which\nstands for quantiles.\n\nSince we are interested in quantifying the 95% CI,\nwe will use the values 0.025 and 0.975, which\nleaves 5% of the probability in the outer tails.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntvalues <- qt(c(0.025, 0.975), 15) ## 15 degrees of freedom (n - 1)\ntvalues\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] -2.13145  2.13145\n```\n\n\n:::\n:::\n\n\nUsing the t-values either from the table or from\nthe `qt()` function, we can compute the confidence\nintervals.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nupper.boundary <- 13.8 + 2.13 * (2.1/sqrt(16))\nlower.boundary <- 13.8 - 2.13 * (2.1/sqrt(16))\n\nupper.boundary\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 14.91825\n```\n\n\n:::\n\n```{.r .cell-code}\nlower.boundary\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 12.68175\n```\n\n\n:::\n:::\n\n\n\n## Looking ahead\n\n\nThis unit provided in-depth theory of sample\nstatistics and we did a lot of the heavy lifting\ncomputations by hand. As I have been showing you,\nR has functions that can compute these quantities\nof interest very quickly. However, understanding\nwhere those computation come from, the assumptions\nmade when running these computations, and how they\ncan fail is just as important as learning how to\nuse them.\n\nIn the next unit, we will use these concepts to\nanalyze experimental data in a side-by-side trial.\nThe concepts of the $t$ distribution and\nconfidence intervals are going to help us look for\ndifference among treatments.\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}