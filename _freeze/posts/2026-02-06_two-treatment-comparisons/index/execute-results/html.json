{
  "hash": "c95f9d2cf606fdf518c87a620dce5b22",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Comparisons between two treatments\"\nsubtitle: \"AGRON 5130 - Module 4\"\nauthor: \"Caio dos Santos\"\ndate: \"2026-02-05\"\noutput:\n  html_document:\n    toc: true\n    toc_float: true\n    theme: flatly\n    highlight: tango\n    smooth: false\n---\n\n\n\n\n\n\n# What have we learned so far?\n\n- So far, we have spent a long time learning\nabout **populations** and how to use the normal\ndistribution model to represent them. We learned\nhow to compute probabilities, build confidence\nintervals, and make statements about the\n**population** mean ($\\mu$).\n- We have also looked at how we cannot always\ncollect all the **population** data, which forces\nus to rely on **samples**.\n- Then, since we estimate the **population**\nvariability ($\\sigma$) using the **sample**\nvariability ($s$), we use the\n$t$-distribution, which approximates more and more\nfrom the normal distribution as we get more\nsamples.\n- Using sample data, we learned how to compute the\nstandard error of the mean, build confidence\nintervals, and make statements about the\n**population** mean using the **samples**.\n\nAlthough I enjoy this, I understand if you are\nrestless and wondering **\"how am I going to use\nany of this in the real world?\"**\n\nUp to this point, we focused on estimating one\npopulation mean. Now we move to the question\nagronomists ask most often: comparing treatments.\n\n\n# Comparing two populations\n\n## What are we really asking?\n\nVery often in agronomy (and in other fields as\nwell), we will want to compare two populations to\nhelp us make informed decisions. For example, we\nmight want to compare a given fertilizer source,\nor whether a given corn hybrid outperforms a\ntraditionally grown corn hybrid consistently.\nAdditionally, we might ask whether the yield\ndifference of a given transgenic trait is worth\nthe added cost.\n\n\nWhile all of these questions (and many more!) can\nbe asked, we can think of a framework that can\ngeneralize these questions. In terms of\nstatistics, what we are really asking is: **is the\nexpected value of one population different from\nthe other's?**\n\n## What do we expect from these populations?\n\nThere is a more formal definition of *expected\nvalue*, but, intuitively, we can think of what you\nwould expect to observe in the **population**. In\na bell-shaped population like the normal\ndistribution, this value would be the\n**population** mean ($\\mu$).\n \nIn the graph below, you can see an example with\ncorn yield values for two populations. Population\n1 has a mean of 180 bu/ac, while population 2 has a\nmean of 210 bu/ac. You can think of populations 1\nand 2 as coming from two different treatments.\nThese can be fertilizer and no fertilizer,\nfungicide and no fungicide, or hybrid A versus\nhybrid B.\n\nSo, when we compare these two populations, we are\nreally asking: \n \n$$H_0: \\mu_1 = \\mu_2 \\newline \nor \\newline\nH_a: \\mu_1 \\ne \\mu_2\n$$\n\nAnd this is a **hypothesis testing** problem. \n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-1-1.png){width=672}\n:::\n:::\n\n\n\n## Hypothesis testing\n\nWe already saw a little bit of hypothesis testing\nin the end of last week's module, when we tried to\nget a bonus from our boss showing that we did get\na yield greater than 12 t/ha, remember?\n\nNow, let's look at the structure for this type of test:\n\n1. We formulate the hypothesis we want to test — in this case, whether the two population means differ ($H_a$).\n2. Then, we formulate what is the *null* hypothesis ($H_0$), which is mutually exclusive with the hypothesis we are testing.\n3. We collect the data and test whether we can reject the *null* hypothesis. When we can, this provides evidence for the alternative hypothesis, but we can never prove that the alternative hypothesis is true.\n\nThere are hundreds of tests that can be used to\ntest hypotheses. Here, we will focus on the\npaired $t$-test because it is designed to\ntest the difference between two groups, which is a\ncommon agronomic problem.\n\n## The paired $t$-test\n\nOne way of looking for the difference in two\npopulations is to look at the distribution of that\ndifference. Translating this to the hypothesis testing\nframework, this becomes:\n\n$$H_0: \\mu_1 = \\mu_2 \\rightarrow \\mu_1 - \\mu_2  =0 \\newline \nH_a: \\mu_1 \\ne \\mu_2\\rightarrow \\mu_1 - \\mu_2   \\ne 0$$\n\n\n### Case study\n\nThe paired $t$-test allows us to compute some of\nthe same statistics as we did in last class\n(t-value and confidence interval) for the\ndifference between the plots within a pair. Let's\ntake a look at a trial in central Ohio in which\nthey tested a starter fertilizer versus without\nthe starter. Hopefully, then, the concept of a\npaired design will be clearer.\n\n#### Looking at the data\n\nIn the figure below, you can see that going\nleft-to-right, we have different pairs of\nexperimental units (which are distributed up and\ndown). In this case, the experimental units are\nrandomized only within each pair (which the\ntextbook calls a block).\n\nThis imposed structure is what allows us to\nconduct the paired $t$-test. **If these\nexperimental units had been distributed randomly\nthroughout the field, this would not be an\nappropriate test.**\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n:::\n\n\nLet's take a look at the data. As we can see, each block has a pair of plots inside. \n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n  block plot treatment yield\n1     1   11   Starter 193.4\n2     1   12   Control 194.2\n3     2   21   Starter 192.2\n4     2   22   Control 189.0\n5     3   31   Control 193.8\n6     3   32   Starter 194.2\n```\n\n\n:::\n:::\n\n\nLet's take a look at the yield value of these\ntreatments. In the plot below, the open circles\nrepresent the different observations for each\ntreatment, and the triangle and error  bar\nrepresent the sample mean plus or minus the\nstandard error.\n\nIt seems like, on average, the yield when using\nthe started was indeed greater. However, how can\nwe test this hypothesis?\n\nHere is the code for reference. I think it is important to be exposed to this ggplot syntax :)\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(corn)+\n  geom_point(aes(x = yield, y = treatment), pch = 1)+\n  labs(x = 'Yield (bu/ac)', y = 'Treatment')+\n  stat_summary(aes(x = yield, y = treatment),\n               pch = 17,\n               fun.data = 'mean_se')+\n  theme_bw()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n:::\n\n\n#### Computing differences\n\nWe want to test whether the mean difference\nbetween these treatments is zero. Let's begin by\ncomputing the differences within each pair.\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n193.4-194.2 = -0.8 \n192.2-189 = 3.2 \n194.2-193.8 = 0.4 \n190.5-186.4 = 4.1 \n198.7-196.4 = 2.3 \n193.4-189.8 = 3.6 \n196-187.6 = 8.4 \n196.7-185.3 = 11.4 \n201.5-184.5 = 17 \n198.9-192.6 = 6.3 \n```\n\n\n:::\n:::\n\n\nLet's put these numbers on a graph so we can have\na better look. It seems like very often, the\nstarter fertilizer provided a yield boost. Once\nagain, the open circles represent the different\nobservations for each treatment, and the triangle\nand error  bar represent the sample mean plus or\nminus the standard error.\n\nOn average, we see a yield increase of 5.6 bu/ac.\nThis shows promise but let's see if, given the\nvariability in this dataset, we would expect to\nsee an increase consistently if we ran this\nexperiment multiple times. You know where this is going, right?\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n:::\n\n\n\n#### Confidence interval\n\nTo build our confidence intervals here, we will need a couple of things:\n\n1. A vector containing the differences\n2. Compute sample mean and standard error\n3. Choose a significance level (e.g., 95% or 99%)\n4. Bring all of these pieces together\n\nHere, we can see a vector in which I collected all the differences within each pair/block.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndifferences\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   1    2    3    4    5    6    7    8    9   10 \n-0.8  3.2  0.4  4.1  2.3  3.6  8.4 11.4 17.0  6.3 \n```\n\n\n:::\n:::\n\n\n\nNext, let's compute the mean and standard error\n\n::: {.cell}\n\n```{.r .cell-code}\nxbar <- mean(differences)\nn <- length(differences)\ns <- sd(differences) / sqrt(n)\n\nxbar\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 5.59\n```\n\n\n:::\n\n```{.r .cell-code}\nn\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 10\n```\n\n\n:::\n\n```{.r .cell-code}\ns\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 1.708895\n```\n\n\n:::\n:::\n\n\nNow, let's check a value of $t$ that would correspond to 95% of the probability distribution area\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntval <- qt(p = c(0.025, 0.975),\n   df = n - 1)\ntval\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] -2.262157  2.262157\n```\n\n\n:::\n:::\n\n\n\nNow, the final step, we bring all of these pieces together and build a confidence interval\n\n\n::: {.cell}\n\n```{.r .cell-code}\nci <- xbar + (tval * s)\nci\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 1.724211 9.455789\n```\n\n\n:::\n:::\n\n\n\n#### Interpreting these numbers\n\nI would like to take a second to interpret these\nnumbers. If we think about it, what do they\nrepresent?\n\nThese numbers indicate that the difference between\nthese two sample means, if the experiment were\nrepeated many many many times, would be between\n1.72 and 9.46 bu/ac. \n\nThe fact that the lower bound does not include 0\nis already an indication of the result of the\n$t$-test, but let's conduct the test to verify.\n\n#### $t$-test\n\nTo conduct the $t$-test, we flip the structure we\nused for the confidence interval. Instead of\nchoosing a t-value that would correspond to a\ncertain significance level, we compute the t\nstatistic, as in last module.\n\n\n$$T = \\frac{\\bar{X} - \\mu}{\\frac{S}{\\sqrt{n}}}$$\n\n\nIn this case, let's compute it for this dataset.\n\n\n$$T = \\frac{\\bar{X} - \\mu}{\\frac{S}{\\sqrt{n}}} = \\frac{5.59 - 0}{1.71} = 3.27$$\n\nThen, we can check using the `fastGraph` library.\nThis shows us that the probability of us observing\na t-statistic value more extreme than the one we\nobserved (3.27) would be very small (< 0.01).\n\nThinking about the hypothesis testing scenario, we\ncould say that there is strong evidence to reject\nthe null hypothesis ($H_0 : \\mu_1 - \\mu_2 = 0 $).\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-11-1.png){width=672}\n:::\n:::\n\n\n\n### R built-in methods\n\nSince R is a programming language with a strong inclination towards statistics, it already has pre-built methods for this.\n\nThe `t.test()` function is what we would use in this case. The use of this function would go something like this.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nt.test(x = VECTOR WITH GROUP 1 DATA,\n       y = VECTOR WITH GROUP 2 DATA,\n       paired = TRUE)\n```\n:::\n\n\n\n#### Reshaping the data\n\nSince we have to split this data frame between two\nvectors, this requires wrangling the data a little\nbit. We have to take the data from a long format,\nto a wide format. In this case, this is important\nto make sure that we do not mix up the yield data\nbetween blocks.\nI will not show how to do this\nhere, as it would be distracting. We have an\nexercise dedicated to this. \n\n\nWe have to go from this:\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n   block plot treatment yield\n2      1   12   Control 194.2\n1      1   11   Starter 193.4\n4      2   22   Control 189.0\n3      2   21   Starter 192.2\n5      3   31   Control 193.8\n6      3   32   Starter 194.2\n8      4   42   Control 186.4\n7      4   41   Starter 190.5\n9      5   51   Control 196.4\n10     5   52   Starter 198.7\n11     6   61   Control 189.8\n12     6   62   Starter 193.4\n14     7   72   Control 187.6\n13     7   71   Starter 196.0\n15     8   81   Control 185.3\n16     8   82   Starter 196.7\n18     9   92   Control 184.5\n17     9   91   Starter 201.5\n19    10  101   Control 192.6\n20    10  102   Starter 198.9\n```\n\n\n:::\n:::\n\nTo this:\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n   block yield.Control yield.Starter\n2      1         194.2         193.4\n4      2         189.0         192.2\n5      3         193.8         194.2\n8      4         186.4         190.5\n9      5         196.4         198.7\n11     6         189.8         193.4\n14     7         187.6         196.0\n15     8         185.3         196.7\n18     9         184.5         201.5\n19    10         192.6         198.9\n```\n\n\n:::\n:::\n\n\n\n#### Running a t-test\n\nNow that our dataset has been reshaped, we can\nconfidently run a paired $t$-test. Note that you\nhave to specify within the function that the\ndesign was paired.\n\nJust like that, within one function, we get the\nsame values we've just manually calculated.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nt.test(corn_wide$yield.Starter,\n       corn_wide$yield.Control,\n       paired = TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tPaired t-test\n\ndata:  corn_wide$yield.Starter and corn_wide$yield.Control\nt = 3.2711, df = 9, p-value = 0.009665\nalternative hypothesis: true mean difference is not equal to 0\n95 percent confidence interval:\n 1.724211 9.455789\nsample estimates:\nmean difference \n           5.59 \n```\n\n\n:::\n:::\n\n\n\n## What about a not paired case?\n\nWhen we do not have a paired case, we cannot\nsimply compute the differences within a pair and\ncompute the t-test of that difference. There's a\nslightly more involved calculation.\n\nWhat I aim to do here is to help you identify when\nyou should use the paired and the common\n\"two-sample $t$-test\". \n\nIn this case, a non-paired $t$-test would be\nappropriate if the experimental units were free to\nvary within the design. In other words, if we had\nnot restricted the design to guarantee that each\nblock/pair (going in the x-axis in the image\nbelow), had both treatments, we would use a\nnon-paired $t$-test.\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-16-1.png){width=672}\n:::\n:::\n\n\n\n#### Running a non-paired $t-test$\n\nAlthough we could go over the calculations for the\nnon-paired $t$-test, I believe we do not gain much\nfrom investing the time in it. The paired case is\nmuch more common in agronomic setting with two\ntreatments. Partly because pairing experimental\nunits saves a lot of headaches with within-field\nvariability.\n\nNon-paired designs, in general, will be those with\nmore than two treatments, and we will discuss\nthose in the coming weeks!\n\nTo run the t-test in R, we can simply change the\n**paired** argument to FALSE. Now, observed that\nthe output is quite different.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nt.test(corn_wide$yield.Starter,\n       corn_wide$yield.Control,\n       paired = FALSE)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tWelch Two Sample t-test\n\ndata:  corn_wide$yield.Starter and corn_wide$yield.Control\nt = 3.3032, df = 17.444, p-value = 0.004087\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n 2.026469 9.153531\nsample estimates:\nmean of x mean of y \n   195.55    189.96 \n```\n\n\n:::\n:::\n\n\n# Conclusion\n\nIn this module, we moved from describing a single\npopulation to answering a much more practical\nquestion: are two treatments actually different?\nAlong the way, you saw how the same tools we have\nalready learned — standard error, degrees of\nfreedom, the $t$-distribution, confidence\nintervals, and hypothesis tests — all come\ntogether to help us make that decision. Whether we\ncompute the differences within pairs or use R’s\nbuilt-in `t.test()` function, the logic is the\nsame: we compare the observed difference to the\nvariability in our data and ask whether that\ndifference is larger than what we would expect\nfrom random chance alone. In agronomy, this is the\ncore of how we evaluate fertilizers, hybrids,\ntraits, and management practices. From here on,\nmost of our analyses will build on this same idea\nof comparing means — just with more treatments and\nslightly more complex designs.\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}