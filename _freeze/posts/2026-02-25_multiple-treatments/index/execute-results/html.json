{
  "hash": "89d295fb7d6bf349d4b456492457107a",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Multiple Treatment Trials\"\nsubtitle: \"Module 6\"\nauthor: \"Caio dos Santos\"\ndate: \"2026-02-23\"\ncategories: [statistics]\n---\n\n\n\n# Introduction\n\nUp to this point, we have focused on experiments with **two treatments**.\n\nBut most agronomic experiments involve more than two treatments:\n\n-   Multiple nitrogen rates\\\n-   Several hybrids\\\n-   Several fungicides\\\n-   Different seeding rates\n\nSo the question becomes:\n\n> What changes when we move from two treatments to many?\n\n# Case Study\n\nSuppose we are testing four seed treatments:\n\n1.  No chemical treatment (Control)\n2.  Insecticide treatment (I)\n3.  Fungicide treatment (F)\n4.  Insecticide + Fungicide (I + F)\n\n## Experimental Layout\n\nThis study was conducted with the 4 treatments mentioned previously and 8 replications, which brings the total number of experimental units to 32. In this case, all experimental units were randomly placed in the field. As the plot layout below shows.\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-1-1.png){width=672}\n:::\n:::\n\n\n\n::: {.cell}\n\n:::\n\n\n## Visualizing the Data\n\nLet's take a look at the data from this trial. Like we've done in previous modules, let's plot the individual observations as empty circles, and then represent the mean and standard error as filled triangles and error bars, respectively.\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n:::\n\n\nWe can see that we have four sample means and their standard errors, each of them represents a different population that we want to make a statement about. Like we've seen in previous modules, each set of sample mean and standard deviation represents a population. These populations can be represented in the distribution plot below:\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n:::\n\n\n## Research question:\n\n> Does seed treatment affect corn yield?\n\n## Hypotheses\n\nNull hypothesis:\n\n$$H_0: \\mu_{control} = \\mu_{I}= \\mu_{F}= \\mu_{I + F}$$\n\nAlternative hypothesis:\n\n$$H_a: at~least~one~mean~is~different$$\n\nNote that the alternative hypothesis does not tell us which of the means is different from the others. It simply states that at least one of them is. Figuring out which mean differs, in case we reject $H_0$, is a separate step that we will explore in a future class.\n\n------------------------------------------------------------------------\n\n## The Linear Additive Model\n\nThe linear additive model is the same as before, just with more treatments:\n\n$$Y_{ij} = \\mu + \\tau_i + \\epsilon_{ij}$$\n\nWhere $i$ indexes the treatment (control, insecticide only, fungicide only, insecticide + fungicide), and $j$ indexes the observation for that treatment (replication).\n\nWhere:\n\n$Y_{ij}$ = observed yield\n\n$\\mu$ = overall mean yield\n\n$\\tau_i$ = treatment effect\n\n$\\varepsilon_{ij}$ = random error\n\nSince the linear additive model decomposes the population means into an overall mean ($\\mu$) and treatment effects ($\\tau_i$), our hypotheses can be restated as:\n\n$$H_0: \\tau_{control} = \\tau_{I}= \\tau_{F}= \\tau_{I + F} = 0$$\n\n$$H_a: at~least~one~\\tau_i \\neq 0$$\n\nThis leads to an important question:\n\n> How do we test this hypothesis?\n\n------------------------------------------------------------------------\n\n# Testing for differences across multiple populations\n\nOne could simply ask:\n\n> Can't we simply run multiple $t$-tests? It worked for two populations!\n\n## Multiple $t$-tests\n\nLet's think about what comparing multiple populations with $t$-tests means.\n\n### Error rate\n\n$\\alpha$ controls the error rate per test, which means that, as the number of tests grows, so does the probability that we will identify false positives in comparisons.\n\nThe table below shows the probability of finding at least one false positive effect (Type I Error) by the number of comparisons we make for two $\\alpha$ values.\n\n| Number of tests | $\\alpha=0.05$ | $\\alpha=0.01$ |\n|:----------------|:--------------|:--------------|\n| 1               | 0.05          | 0.01          |\n| 3               | 0.14          | 0.030         |\n| 6               | 0.27          | 0.06          |\n| 10              | 0.40          | 0.1           |\n| 45              | 0.90          | 0.36          |\n\nSome observations about this table:\n\n-   Lower $\\alpha$ values can help us control this experiment-wise error rate. However, remember the trade-off between $\\alpha$ and power to identify differences?\n-   Lowering $\\alpha$ too much might hurt our ability to identify differences unless they're large.\n-   At 45 comparisons, we are almost guaranteed to make a mistake.\n\nLooking at these numbers, you might be asking how often do we make 45 comparisons?\n\n### Number of comparisons\n\nLet's explore the relationship between the number of treatments and the number of comparisons in a trial.\n\n#### 2 treatments\n\nIf we are comparing treatments **A** and **B**, the means we might think about are $\\mu_A$ and $\\mu_B$.\n\n1.  $\\mu_A = \\mu_B$\n\n#### 3 Treatments\n\n**A, B, and C**\n\nWe run the following comparisons:\n\n1.  $\\mu_A = \\mu_B$\\\n2.  $\\mu_B = \\mu_C$\\\n3.  $\\mu_A = \\mu_C$\n\n#### Visualizing this relationship\n\nThe number of comparisons can be computed as $K\n\\times (K - 1)/2$ and can be visualized below:\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n:::\n\n\nIf we ran all pairwise t-tests, we would inflate our Type I error rate. Instead, we need a different approach. In this context, the **Analysis of Variance (ANOVA)** emerges as a solution.\n\n------------------------------------------------------------------------\n\n# Analysis of Variance (ANOVA)\n\nWhen we are talking about testing differences in means, it must sound strange to talk about analyzing variances. Let's take a look at the rationale behind this analysis.\n\nJust like before, we are looking to test the ratio between signal and noise. In the ANOVA, we partition the total variance into **variance between treatments** (signal) and **variance within treatments** (noise).\n\nThis is done using the $F$-statistic.\n\n$$F = \\frac{\\sigma^2_{treatment}}{\\sigma^2_{error}}$$\n\nOnce again, since we do not know the population variances, we estimate those using the sample data.\n\nTo do so, we will partition the variance:\n\n$$\\sigma^2_{total} = \\sigma^2_{treatment} + \\sigma^2_{error}$$ In the case of samples, we estimate the variance using the Mean Square Error:\n\n$$MSE = \\frac{(Y_i - \\bar{Y})^2}{n - 1} = \\frac{SS}{df}$$ Where,\n\n$Y_i$ is an observed value\n\n$\\bar{Y}$ is the mean observed value\n\n$n$ is the number of observations\n\nYou may recognize the numerator and denominator as sum of squared errors, and degrees of freedom, respectively.\n\nLike we did before, let's decompose these into the linear additive model terms ($\\mu,\\tau,\\epsilon$), so it's easier to visualize them.\n\n\n::: {.cell}\n\n:::\n\n\nLet's compute first $\\mu$. This is an easy one:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmu <- mean(seed_trial$yield)\nmu\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 169.4462\n```\n\n\n:::\n:::\n\n\nNext, let's compute the treatment means and extract $tau_i$.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntaus <- aggregate(seed_trial$yield,\n                  seed_trial['trt'],\n                  mean)\nnames(taus) <- c('trt', 'trt_means')\ntaus$mu <- mu\ntaus$tau <- taus$trt_means - taus$mu\ntaus\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n      trt trt_means       mu        tau\n1 Control  154.0178 169.4462 -15.428439\n2       F  165.7060 169.4462  -3.740163\n3       I  171.3126 169.4462   1.866396\n4   I + F  186.7484 169.4462  17.302206\n```\n\n\n:::\n:::\n\n\nOk! We are much closer to computing all effects, we just need the errors ($\\epsilon$) now. To do that, let's merge the dataset that contains the two effects we've computed so far ($\\tau_i, \\mu$), with the dataset containing all observations, as the errors ($\\epsilon_i$) are computed for every observation.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nseed_trial_eff <- merge(seed_trial, taus)\nseed_trial_eff$epsilon <- with(seed_trial_eff, yield - mu - tau)\nhead(seed_trial_eff)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n      trt    yield trt_means       mu       tau     epsilon\n1 Control 153.5004  154.0178 169.4462 -15.42844 -0.51738435\n2 Control 148.8587  154.0178 169.4462 -15.42844 -5.15905744\n3 Control 148.5945  154.0178 169.4462 -15.42844 -5.42323632\n4 Control 153.9684  154.0178 169.4462 -15.42844 -0.04932593\n5 Control 162.5157  154.0178 169.4462 -15.42844  8.49791715\n6 Control 148.5269  154.0178 169.4462 -15.42844 -5.49087801\n```\n\n\n:::\n:::\n\n\nTo make it easier for us to visualize these effects, let's plot them:\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-10-1.png){width=672}\n:::\n:::\n\n\n## Setting up the ANOVA Table\n\nGreat! Now we have all the effects we need to compute the $F$-statistic!\n\nI will set up a table for us to keep track of these numbers and we will populate this table as we go:\n\n| Source of Variation | SS  | DF  | MSE | F   |\n|:--------------------|:----|:----|:----|:----|\n| Treatment           |     |     |     |     |\n| Residuals (Error)   |     |     |     |     |\n| Total               |     |     |     |     |\n\n## Sums of Squares\n\nLet's compute the Sums of Squares for all three rows\n\n\n::: {.cell}\n\n```{.r .cell-code}\nSS_treatment <- sum(seed_trial_eff$tau ^ 2) ## Treatment\nSS_error <- sum(seed_trial_eff$epsilon ^ 2) ## Error\nSS_total <- sum((seed_trial_eff$yield - seed_trial_eff$mu)^2) ## Total\n\nSS_treatment\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 4439.003\n```\n\n\n:::\n\n```{.r .cell-code}\nSS_error\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 1781.786\n```\n\n\n:::\n\n```{.r .cell-code}\nSS_total\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 6220.788\n```\n\n\n:::\n:::\n\n\nNow, let's add these values to our table:\n\n| Source of Variation | SS   | DF  | MSE | F   |\n|:--------------------|:-----|:----|:----|:----|\n| Treatment           | 4439 |     |     |     |\n| Residuals (Error)   | 1782 |     |     |     |\n| Total               | 6220 |     |     |     |\n\nGreat! Let's continue populating our table. Next, let's compute the degrees of freedom!\n\n## Degrees of Freedom\n\nWe have a total of 32 observations, and we are estimating 4 means (one for each treatment)\n\nWe can compute the total number of degrees of freedom as:\n\n$DF_{total}$ = 32 - 1 = 31\n\nThe degrees of freedom for the treatment can be computed as $K - 1$, where K is the number of means we are estimating.\n\n$DF_{treatment}$ = 4 - 1 = 3\n\nThe degrees of freedom for the error, is simply the difference between the total degrees of freedom, and the degrees of freedom taken by the treatment:\n\n$DF_{error}$ = 31 - 3 = 28\n\nOur table now becomes:\n\n| Source of Variation | SS   | DF  | MSE | F   |\n|:--------------------|:-----|:----|:----|:----|\n| Treatment           | 4439 | 3   |     |     |\n| Residuals (Error)   | 1782 | 28  |     |     |\n| Total               | 6220 | 31  |     |     |\n\n## Mean Square Error\n\nNext, we need to take the ratio between the SS and DF columns. Remember, this will give us an estimate for the variance of each effect. This estimate is called Mean Square Error (MSE).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nMS_treatment <- SS_treatment/3 # MSE Treatment\nMS_error <- SS_error/28 # MSE Residuals\n\nMS_treatment\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 1479.668\n```\n\n\n:::\n\n```{.r .cell-code}\nMS_error\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 63.6352\n```\n\n\n:::\n:::\n\n\n| Source of Variation | SS   | DF  | MSE    | F   |\n|:--------------------|:-----|:----|:-------|:----|\n| Treatment           | 4439 | 3   | 1479.7 |     |\n| Residuals (Error)   | 1782 | 28  | 63.6   |     |\n| Total               | 6220 | 31  |        |     |\n\n## Computing the $F$-statistic\n\nNow, since the $F$-statistic is computed as the ratio between the treatment and error variances, and we are estimating them using the MSE, $F$ can be computed as follows:\n\n$$F = \\frac{\\sigma^2_{treatment}}{\\sigma^2_{error}} = \\frac{MSE_{treatment}}{MSE_{error}}$$\n\n\n::: {.cell}\n\n```{.r .cell-code}\nMS_treatment / MS_error\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 23.25234\n```\n\n\n:::\n:::\n\n\nIf treatments truly do not differ, both MS terms estimate the same variance, and F should be close to 1. An F-value of 23 means the treatment variance is 23 times larger than the background noise.\n\nAlright! This concludes our ANOVA table!!\n\n| Source of Variation | SS   | DF  | MSE    | F     |\n|:--------------------|:-----|:----|:-------|:------|\n| Treatment           | 4439 | 3   | 1479.7 | 23.25 |\n| Residuals (Error)   | 1782 | 28  | 63.6   |       |\n| Total               | 6220 | 31  |        |       |\n\n## P-value\n\nNow, like we have done before, we have to compare the $F$-statistic value we found with the $F$-distribution and compute the probability of observing a more extreme value given that $H_0$ is true.\n\nWe will use the `fastGraph` library for that once again:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(fastGraph)\nshadeDist(xshade = 23.25,\n          ddist = 'df',\n          parm1 = 3,\n          parm2 = 28,\n          lower.tail = FALSE)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-14-1.png){width=672}\n:::\n:::\n\n\nThis low p-value would provide strong evidence for us to reject $H_0$, indicating that at least one of the treatment means is different from the others.\n\n## R built-in function\n\nLike before, R has built-in function to run ANOVAs. However, I think it is important for us to build an ANVOA table step-by-step at least once, so we understand what each piece of information represents.\n\nLet's run the built-in function:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel1 <- aov(yield ~ trt, data = trial)\nsummary(model1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n            Df Sum Sq Mean Sq F value   Pr(>F)    \ntrt          3   4439  1479.7   23.25 9.28e-08 ***\nResiduals   28   1782    63.6                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n:::\n\n\n# Conclusion\n\nWhen we move from two treatments to many, the logic of hypothesis testing does not change, but the mechanics do. Instead of comparing one mean to another with a $t$-test, we partition the total variability in the data into variability due to treatments and variability due to random error. ANOVA works by comparing these two sources of variation through the $F$-statistic. If treatments truly have no effect, both sources estimate the same background variance, and the $F$ value will large relative to experimental noise, the treatment mean square becomes much larger than the error mean square, and the $F$ value increases. This allows us to test all treatment means simultaneously while maintaining control over our Type I error rate.\n\nIn the future, we will look at experimental designs with multiple treatments, and how to compare treatment means to make statements about which means are different from which.\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}