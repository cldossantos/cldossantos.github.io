[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "",
    "section": "",
    "text": "I have received my Ph.D. from the Department of Agronomy at Iowa State University with a Minor in Statistics. My research integrates crop physiology, digital agriculture, and statistical modeling to improve management and quantification of spatio- temporal variability in cropping systems. Much of my research has centered on physiological responses to nutrient availability, photoperiod, temperature, and genotype-by-environment interactions. I have worked extensively on tissue analysis, canopy dynamics, and phenological development in corn, soybean, and cover crops."
  },
  {
    "objectID": "about.html#about",
    "href": "about.html#about",
    "title": "",
    "section": "",
    "text": "I have received my Ph.D. from the Department of Agronomy at Iowa State University with a Minor in Statistics. My research integrates crop physiology, digital agriculture, and statistical modeling to improve management and quantification of spatio- temporal variability in cropping systems. Much of my research has centered on physiological responses to nutrient availability, photoperiod, temperature, and genotype-by-environment interactions. I have worked extensively on tissue analysis, canopy dynamics, and phenological development in corn, soybean, and cover crops."
  },
  {
    "objectID": "about.html#research-interests",
    "href": "about.html#research-interests",
    "title": "",
    "section": "Research interests",
    "text": "Research interests\nThere are several research areas that fascinate me. A short list of those would be: crop physiology, crop models, remote sensing, soil fertility, and statistical models."
  },
  {
    "objectID": "about.html#education",
    "href": "about.html#education",
    "title": "",
    "section": "Education",
    "text": "Education\n\nPh.D. in Crop Production and Physiology\n\nMinor in Statistics\nIowa State University\n2020 - 2025\n\nM.Sc. in Crop, Soil, and Environmental Sciences\n\nUniversity of Arkansas\n2018 - 2020\n\nB.Sc. in Agronomy\n\nCollege of Agriculture “Luiz de Queiroz”\nUniversity of Sao Paulo\n2013 - 2018"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Posts",
    "section": "",
    "text": "Experimental Design\n\n\n\n\n\n\n\n\n\n\n\nDec 3, 2025\n\n\nCaio dos Santos\n\n\n\n\n\n\n\n\n\n\n\n\nProcessing as-applied and as-planted data in on-farm trials\n\n\n\n\n\n\n\n\n\n\n\nJul 19, 2025\n\n\nCaio dos Santos\n\n\n\n\n\n\n\n\n\n\n\n\nProcessing as-applied and as-planted data in on-farm trials\n\n\n\n\n\n\n\n\n\n\n\nJul 19, 2025\n\n\nCaio dos Santos\n\n\n\n\n\n\n\n\n\n\n\n\nSpecifying custom vegetation indices\n\n\n\n\n\n\n\n\n\n\n\nMay 13, 2025\n\n\nCaio dos Santos\n\n\n\n\n\n\n\n\n\n\n\n\nProcessing data from an on-farm experiment\n\n\n\n\n\n\n\n\n\n\n\nMar 13, 2025\n\n\nCaio dos Santos\n\n\n\n\n\n\n\n\n\n\n\n\nWorking with yield monitor data\n\n\n\n\n\n\n\n\n\n\n\nMar 10, 2025\n\n\nCaio dos Santos\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2025-05-13_vegetation-indices/index.html",
    "href": "posts/2025-05-13_vegetation-indices/index.html",
    "title": "Specifying custom vegetation indices",
    "section": "",
    "text": "Part of the motivation for developing pacu was to facilitate the access to and the analysis of satellite data. Up until now, users could only use one of the six pre-specified vegetation indices within pa_compute_vi(). In an effort to make the package more comprehensive, we have included the option of specifying custom vegetation indices. This gives users the freedom to go beyond the predefined vegetation indices and tailor their analysis to their specific needs! Here, we will go through an example of how to download and process satellite data using pacu. Let’s get started!"
  },
  {
    "objectID": "posts/2025-05-13_vegetation-indices/index.html#introduction",
    "href": "posts/2025-05-13_vegetation-indices/index.html#introduction",
    "title": "Specifying custom vegetation indices",
    "section": "",
    "text": "Part of the motivation for developing pacu was to facilitate the access to and the analysis of satellite data. Up until now, users could only use one of the six pre-specified vegetation indices within pa_compute_vi(). In an effort to make the package more comprehensive, we have included the option of specifying custom vegetation indices. This gives users the freedom to go beyond the predefined vegetation indices and tailor their analysis to their specific needs! Here, we will go through an example of how to download and process satellite data using pacu. Let’s get started!"
  },
  {
    "objectID": "posts/2025-05-13_vegetation-indices/index.html#data-acquisition-and-processing",
    "href": "posts/2025-05-13_vegetation-indices/index.html#data-acquisition-and-processing",
    "title": "Specifying custom vegetation indices",
    "section": "Data acquisition and processing",
    "text": "Data acquisition and processing\nWe will use two libraries for this example, pacu and sf. In case you do not have them installed, you can install them using the following code chunk\n\ninstall.packages('sf')\n\n## we will install pacu from GitHub because of some newly added features\n## version 0.1.63\nremotes::install_github('cldossantos/pacu')\n\nLet’s load the libraries:\n\nlibrary(sf)\nlibrary(pacu)\n\nIn this example, we’ll focus on the Ada Hayden Heritage Park. I will set up the corners here so you do not have to download any files to be able to follow along.\n\ncorners &lt;- matrix( c(-93.639, 42.0758,\n                     -93.6214, 42.0621), \n                   ncol = 2,\n                   byrow = TRUE)\n\ncorners &lt;- st_multipoint(corners)\n\n## defining our area of interest\naoi &lt;- st_as_sf(st_as_sfc(st_bbox(corners)),\n                crs = 4326)\n\nWe can search for images of the park over a given period and filter them based on cloud coverage. Here, we look for images from May of 2024 and with a maximum cloud coverage of 10%.\nnote: there is a registration step required before you can download images from Copernicus Data Space, please read the vignette.\n\navailable.images &lt;- pa_browse_dataspace(aoi, \n                                        start.date = '2024-05-13',\n                                        end.date = '2024-05-19',\n                                        max.cloud.cover = 10)\n\nWe have found one image matching our search parameters:\n\navailable.images\n\nSearch parameters\nStart date: 2024-05-13 \nEnd date: 2024-05-19 \nMax. cloud cover: 10%\nCollection name:  SENTINEL-2 \n\nResults\nTotal:  1 \nOnline:  1 \n\n\nDownloading the image can take some time. On my machine, it took about 7 minutes. Additionally, the image is quite large and can take up a lot of space on the computer. Providing the aoi argument ensures that pacu will crop the downloaded image to the area of interest. This can save a lot of storage space when working with several images.\n\ndownloaded.images &lt;- pa_download_dataspace(available.images,\n                                           dir.path = './raw-data/',\n                                           aoi = aoi,\n                                           verbose = FALSE) ## suppressing the progressbar\n\nHere, we can take a look at the true color image. It is actually a really pretty lake!\n\nrgb &lt;- pa_get_rgb(downloaded.images,\n                  verbose = FALSE)\npa_plot(rgb)\n\n\n\n\n\n\n\n\nOne of the pre-built vegeation indices within pacu is the Normalized Difference Vegetation Index (NDVI). This VI is often used as an indicator of crop health but in this case, we can see that it also shows us the lake quite well.\n\nndvi &lt;- pa_compute_vi(downloaded.images,\n                      vi = 'ndvi',\n                      verbose = FALSE)\n\npa_plot(ndvi)"
  },
  {
    "objectID": "posts/2025-05-13_vegetation-indices/index.html#specifying-custom-vegetation-indices",
    "href": "posts/2025-05-13_vegetation-indices/index.html#specifying-custom-vegetation-indices",
    "title": "Specifying custom vegetation indices",
    "section": "Specifying custom vegetation indices",
    "text": "Specifying custom vegetation indices\nResearchers might be interested in using other vegetation indices to identify water bodies, for example, such as the Multi-Band Water Index. This can be done by specifying the mathematical relationship between the Sentinel-2 bands in the formula argument of pa_compute_vi().\nIt is important to note that there is a mismatch between the resolution of the bands involved in this computation- some are 10m and others are 20m. The package will recognize that mismatch and resample the finer resolution band to match the coarser resolution ones. This means that the resolution of the output raster is the same as the resolution of the coarsest band involved in the computation.\n\nmbwi &lt;- pa_compute_vi(downloaded.images,\n                      vi = 'other',\n                      formula = mbwi ~ (2 * B03 - B04 - B08 - B11 - B12),\n                      verbose = FALSE)\npa_plot(mbwi, palette = 'Blues') ## blue colors for water!"
  },
  {
    "objectID": "posts/2025-05-13_vegetation-indices/index.html#conclusion",
    "href": "posts/2025-05-13_vegetation-indices/index.html#conclusion",
    "title": "Specifying custom vegetation indices",
    "section": "Conclusion",
    "text": "Conclusion\nWe have seen the process of acquiring and processing satellite data from Sentinel-2, and a new feature within pacu that allows us to specify custom vegetation indices. This gives the user more power to define and explore different indices that might be useful in a different discipline. We’ll be back with more pacu news later!"
  },
  {
    "objectID": "posts/2024-12-16_central-park/index.html",
    "href": "posts/2024-12-16_central-park/index.html",
    "title": "Remote sensing of NYC Central Park",
    "section": "",
    "text": "I thought it would be interesting to look at how to retrieve remotely sensed data from Sentinel-2 using the pacu package. For this example, I decided to take a look at the Central Park in New York city. Although it is not an agricultural setting, it is public large vegetated area.\nThe framework to download, process, and visualize the data for an agricultural field would be nearly identical. The one big difference would be the geographical coordinates. These would have to point to the targeted agricultural field.\nFirst, we can start by loading the libraries we will need for this task.\nlibrary(pacu)\nlibrary(sf)\nIf you have not installed these libraries, you can do so by running\ninstall.packages(\"pacu\")\nNow, we can define our area of interest. In this case, we will define the Central Park as our targeted area.\ncentral.park &lt;- read_sf('./central-park.shp', quiet = TRUE)\nThe first step to retrieve Sentinel-2 data is to register with Copernicus Data Space. Please check the package vignettes -more specifically the satellite data vignette- for more information on registering. You can also check the help page for the function pa_initialize_dataspace().\n?pa_initialize_dataspace\nWe can browse the Data Space catalog and check how many images will meet our search parameters. In this case, I am searching for images covering the Central Park between May and October, with 20% or less cloud coverage.\nWe can see that there are a total of 21 images the meet our criteria.\navailable.images &lt;- pa_browse_dataspace(aoi = central.park,\n                                        start.date = '2023-05-01',\n                                        end.date = '2023-10-30',\n                                        max.cloud.cover = 20)\navailable.images\n\nSearch parameters\nStart date: 2023-05-01 \nEnd date: 2023-10-30 \nMax. cloud cover: 20%\nCollection name:  SENTINEL-2 \n\nResults\nTotal:  23 \nOnline:  23\nThe summary() function can be used to tell us how many images we have available for every month.\nsummary(available.images)\n\n------------------\nYear  Month  Count \n---   ---    ---   \n2023  5      6     \n2023  6      2     \n2023  7      4     \n2023  8      3     \n2023  9      4     \n2023  10     4     \n------------------\nTotal   23\nLet us take a look at an image from May, one from July, and one from October.\nps: I am setting verbose to FALSE from now on to suppress the progress bar.\ndownloaded.images &lt;- pa_download_dataspace(available.images[c(3, 11, 21), ],\n                      aoi = central.park,\n                      dir.path = '.',\n                      verbose = FALSE)\nWe can look at a true color image using the pa_get_rgb() function.\ntrue.color &lt;- pa_get_rgb(downloaded.images, \n                         verbose = FALSE)\npa_plot(true.color)\nAlternatively, we can take a look at vegetation indices, such as the Normalized Difference Vegetation Index (NDVI).\nndvi &lt;- pa_compute_vi(downloaded.images, \n                      vi = 'ndvi',\n                      aoi = central.park,\n                      check.clouds = TRUE,\n                      verbose = FALSE)\npa_plot(ndvi)\nWe cal also look at the Normalized Difference Red Edge (NDRE) index. We can see that the image is at a coarser resolution, when compared to the NDVI image. This is because the Red Edge band in Sentinel-2 is at a 20m resolution, while the bands involved in the NDVI computation are at a 10m resolution.\nndre &lt;- pa_compute_vi(downloaded.images, \n                      vi = 'ndre',\n                      aoi = central.park,\n                      check.clouds = TRUE,\n                      verbose = FALSE)\npa_plot(ndre)\nWe can also take a look at the timeseries plot of the median NDRE over Central Park in 2023.\nsumm &lt;- summary(ndre, \n                by = central.park,\n                fun = function(x) median(x, na.rm = TRUE))\npa_plot(summ, plot.type = 'timeseries')"
  },
  {
    "objectID": "posts/2024-12-16_central-park/index.html#conclusion",
    "href": "posts/2024-12-16_central-park/index.html#conclusion",
    "title": "Remote sensing of NYC Central Park",
    "section": "Conclusion",
    "text": "Conclusion\nWe have seen how pacu can help us browse, download, and process satellite images for a non-agricultural setting. The workflow for an agricultural field would be nearly identical. We would only need to replace the area.of.interest."
  },
  {
    "objectID": "posts/2025-03-13_on-farm-experiment/index.html",
    "href": "posts/2025-03-13_on-farm-experiment/index.html",
    "title": "Processing data from an on-farm experiment",
    "section": "",
    "text": "In a previous post, I have mentioned that pacu provided support for processing data coming from on-farm trials, as well as production fields. In this post, I want to explore and showcase this functionality a little more in depth. The idea is to provide an example of processing these data and make a few considerations that can greatly impact the end result.\nWe will look at simulated data from a small on-farm trial. This is made up data and I simulated a relationship between corn yield and nitrogen rate that followed a linear plateau relationship like the image below. The simulated relationship presented an intercept = 7000, slope = 30, and a breakpoint = 180. Let’s process this data and see if we can recover this relationship. You can download the data here."
  },
  {
    "objectID": "posts/2025-03-13_on-farm-experiment/index.html#introduction",
    "href": "posts/2025-03-13_on-farm-experiment/index.html#introduction",
    "title": "Processing data from an on-farm experiment",
    "section": "",
    "text": "In a previous post, I have mentioned that pacu provided support for processing data coming from on-farm trials, as well as production fields. In this post, I want to explore and showcase this functionality a little more in depth. The idea is to provide an example of processing these data and make a few considerations that can greatly impact the end result.\nWe will look at simulated data from a small on-farm trial. This is made up data and I simulated a relationship between corn yield and nitrogen rate that followed a linear plateau relationship like the image below. The simulated relationship presented an intercept = 7000, slope = 30, and a breakpoint = 180. Let’s process this data and see if we can recover this relationship. You can download the data here."
  },
  {
    "objectID": "posts/2025-03-13_on-farm-experiment/index.html#working-with-the-data",
    "href": "posts/2025-03-13_on-farm-experiment/index.html#working-with-the-data",
    "title": "Processing data from an on-farm experiment",
    "section": "Working with the data",
    "text": "Working with the data\nLoading the necessary packages\n\nlibrary(pacu)\nlibrary(sf)\nlibrary(nlraa)\nlibrary(nlme)\n\nHere, we will import the data sets into R. We will use two data sets for this exercise. The first contains made up raw yield data that represents data coming from an on-farm trial. The second contains an example of nitrogen rates applied to each experimental unit.\n\nraw.yield &lt;- read_sf('./raw-data/example-raw-data.shp',\n                     quiet = TRUE)\n\ntrial.design &lt;- read_sf('./raw-data/example-trial-design.shp',\n                     quiet = TRUE)\n\nLets take a look at how these two data sets line up in space. In this example, the points represent the yield monitor readings and the red rectangles represent the experimental units. We can see that, in this example, we had about 4 combine passes per experimental unit.\n\nplot(st_geometry(raw.yield), pch = 1, cex = 0.3)\nplot(st_geometry(trial.design), border = 'red', add = TRUE)\n\n\n\n\n\n\n\n\nAnother piece of information that can be extracted from the trial.design data set is the amount of nitrogen applied to each experimental unit in this example.\n\nplot(trial.design[\"nrate\"], main = 'Nitrogen rate (kg/ha)')\n\n\n\n\n\n\n\n\nWe can take a look at the raw yield data to see the kind of data we are dealing with. We can see that there is a lot of variability in the yield data, with it ranging from \\(\\approx 200\\) to \\(\\approx 12000\\) kg/ha. Some of this variability comes from the treatment effect but there’s a part of it that is just random variability. The challenge lies in identifying the treatment effect.\n\nboxplot(raw.yield$yld_kgh,\n        ylab = 'Yield (kg/ha)')\n\n\n\n\n\n\n\nplot(raw.yield[\"yld_kgh\"],\n     pch = 16,\n     main = 'Yield (kg/ha)',\n     breaks = 'quantile',\n     reset = FALSE)\nplot(st_geometry(trial.design),\n     add = TRUE)"
  },
  {
    "objectID": "posts/2025-03-13_on-farm-experiment/index.html#cleaning-based-on-standard-deviation",
    "href": "posts/2025-03-13_on-farm-experiment/index.html#cleaning-based-on-standard-deviation",
    "title": "Processing data from an on-farm experiment",
    "section": "Cleaning based on standard deviation",
    "text": "Cleaning based on standard deviation\nSomething that is common to do, is to use empirical rules to remove outliers and clean some of the noise inherent to these type of data, let’s see the effect of that.\nA pretty common procedure is to clean anything outside of 2 or 3 standard deviations from the mean (you can read more about it here). Let us take a look at this procedure. I will isolate one experimental unit for us to take a closer look. The removed points are marked with a red “X”. For this experimental unit, anything that was smaller tha 5622 or greater than 8139 kg/ha was removed.\n\n## selecting only experimental unit 5\nto.keep &lt;- as.numeric(st_intersects(raw.yield, trial.design[5, ]))\none.eu &lt;- raw.yield[!is.na(to.keep), 'yld_kgh']\n\n## calculating the mean and sd\neu.mean &lt;- mean(one.eu[['yld_kgh']])\neu.sd &lt;- sd(one.eu[['yld_kgh']])\n\n## defining upper and lower boundaries and identifying which\n## data points to remove\nupper.boundary &lt;- eu.mean + 2 * eu.sd\nlower.boundary &lt;- eu.mean - 2 * eu.sd\nto.remove &lt;- (one.eu$yld_kgh &lt; lower.boundary | \n                one.eu$yld_kgh &gt; upper.boundary)\n\n## showing which points were removed\nplot(one.eu, reset = FALSE,\n     main = 'Yield (kg/ha)',\n     pch = 16)\nplot(st_geometry(one.eu)[to.remove],\n     cex = 2,\n     col = 'red',\n     pch = 'x', \n     add = TRUE)\n\n\n\n\n\n\n\n\nWe can now apply this same methodology to all experimental units and see that, in total, we will remove 60 points from this data set.\n\nraw.yield$eu &lt;- as.numeric(st_intersects(raw.yield, trial.design))\nraw.yield &lt;- raw.yield[order(raw.yield$eu), ]\nto.remove &lt;- ave(raw.yield$yld_kgh, \n                 raw.yield$eu,\n                 FUN = function(x){\n                   eu.mean &lt;- mean(x)\n                   eu.sd &lt;- sd(x)\n                   upper.boundary &lt;- eu.mean + 2 * eu.sd\n                   lower.boundary &lt;- eu.mean - 2 * eu.sd\n                    (x &lt; lower.boundary | x &gt; upper.boundary)\n                 })\nto.remove &lt;- as.logical(to.remove)\n\nplot(raw.yield['yld_kgh'], \n     main = 'Yield (kg/ha)',\n     reset = FALSE,\n     pch = 16)\nplot(st_geometry(raw.yield)[to.remove],\n     col = 'red',\n     pch = 'x', \n     add = TRUE)\n\n\n\n\n\n\n\n\nNow, let us remove these data points, and average the yield observations within each experimental unit. We can see that our estimates of the model parameters are not quite the same ones that we simulated. In addition, the variance of the parameter estimates is pretty large. This means that we are not very certain of these values.\n\nyield.filtered.sd &lt;- raw.yield[!to.remove, ]\nmean.yield.sd &lt;- aggregate(yield.filtered.sd['yld_kgh'],\n                           trial.design,\n                           FUN = mean)\nmean.yield.sd &lt;- st_join(mean.yield.sd, trial.design, join = st_equals)\n\n\nfit1 &lt;- nls(yld_kgh ~ SSlinp(nrate, a, b, xs),\n            data = mean.yield.sd)\n# estimates\ncoef(fit1)\n\n         a          b         xs \n6814.09790   28.01518  168.74310 \n\n# variances\ndiag(vcov(fit1))\n\n           a            b           xs \n28833.979455     3.075624    71.175162 \n\n## Visualizing the data\n\npreds &lt;- data.frame(nrate = 0:300)\npreds &lt;- cbind(preds,  predict_nls(fit1,\n                                   interval = 'conf',\n                                   newdata = preds))\n\nplot(mean.yield.sd$nrate, mean.yield.sd$yld_kgh, \n     xlab = 'Nitrogen rate (kg/ha)',\n     ylab = 'Yield (kg/ha)')\npolygon(x = c(preds$nrate, rev(preds$nrate)),\n        y = c(preds$Q2.5, rev(preds$Q97.5) ),\n        border = 'steelblue', lty = 2) \nlines(preds$nrate, preds$Estimate, \n      col = 'blue')\nlegend('bottomright',\n       lty = 1:2,\n       legend = c('Estimate', 'CI'),\n       col = c('blue', 'steelblue'))"
  },
  {
    "objectID": "posts/2025-03-13_on-farm-experiment/index.html#adding-a-buffer-to-the-experimental-units",
    "href": "posts/2025-03-13_on-farm-experiment/index.html#adding-a-buffer-to-the-experimental-units",
    "title": "Processing data from an on-farm experiment",
    "section": "Adding a buffer to the experimental units",
    "text": "Adding a buffer to the experimental units\nAnother empirical cleaning method that is commonly used is to add a buffer to the experimental units. This in meant to remove some of the border effect or the transition between experimental units. However, adding a buffer needs to be done with caution. You want your buffer to be big enough to remove some of these potentially problematic observations, but you do not want your buffer to remove so many points that you will end up estimating the mean with few points. This would increase the uncertainty of your estimate. Let’s take a look at different buffer sizes:\n\nbuffers &lt;- c(1, 2, 5, 10)\ncols &lt;- hcl.colors(4, palette = 'Temps')\nplot(st_geometry(raw.yield), cex = 0.5)\nfor (i in 1:length(buffers)){\n  buffered.exp.units &lt;- st_as_sf(st_buffer(trial.design, -buffers[i]))\n  plot(st_geometry(buffered.exp.units), border = cols[i], add = TRUE)\n}\nlegend('topleft', \n       fill = cols,\n       legend = buffers,\n       title = 'Buffer size (m)')\n\n\n\n\n\n\n\n\nIt seems that a buffer size of about 5 meters is what we want for this example. Let’s go with that and see which observations will be removed from the data set:\n\nbuffered.exp.units &lt;- st_as_sf(st_buffer(trial.design, -5))\nraw.yield$eu &lt;- as.numeric(st_intersects(raw.yield, buffered.exp.units))\nraw.yield &lt;- raw.yield[order(raw.yield$eu), ]\nto.remove &lt;- ave(raw.yield$yld_kgh, \n                 raw.yield$eu,\n                 FUN = function(x){\n                   eu.mean &lt;- mean(x)\n                   eu.sd &lt;- sd(x)\n                   upper.boundary &lt;- eu.mean + 2 * eu.sd\n                   lower.boundary &lt;- eu.mean - 2 * eu.sd\n                    (x &lt; lower.boundary | x &gt; upper.boundary)\n                 })\n\nto.remove &lt;- as.logical(to.remove)\n\nplot(raw.yield['yld_kgh'], \n     main = 'Yield (kg/ha)',\n     reset = FALSE,\n     pch = 16)\nplot(st_geometry(raw.yield)[to.remove],\n     col = 'red',\n     pch = 'x', \n     add = TRUE)\n\n\n\n\n\n\n\n\nNow, let us remove these data points, and average the yield observations within each experimental unit. We can see that our estimates of the model parameters are a little closer to the true values but still not quite there. Also, the variance has decreased quite a bit but let’s see if we can increase the precision of our estimates!\n\nyield.filtered.buffer &lt;- raw.yield[!to.remove, ]\nmean.yield.buffer &lt;- aggregate(yield.filtered.buffer['yld_kgh'],\n                           trial.design,\n                           FUN = mean)\nmean.yield.buffer &lt;- st_join(mean.yield.buffer, trial.design, join = st_equals)\n\n\nfit2 &lt;- nls(yld_kgh ~ SSlinp(nrate, a, b, xs),\n            data = mean.yield.buffer)\n## estimates\ncoef(fit2)\n\n        a         b        xs \n6659.5802   30.3704  172.9306 \n\n## variances\ndiag(vcov(fit2))\n\n          a           b          xs \n6649.190326    0.709247   14.583390 \n\n## Visualizing the data\n\npreds &lt;- data.frame(nrate = 0:300)\npreds &lt;- cbind(preds,  predict_nls(fit2,\n                                   interval = 'conf',\n                                   newdata = preds))\n\nplot(mean.yield.buffer$nrate, \n     mean.yield.buffer$yld_kgh, \n     xlab = 'Nitrogen rate (kg/ha)',\n     ylab = 'Yield (kg/ha)')\npolygon(x = c(preds$nrate, rev(preds$nrate)),\n        y = c(preds$Q2.5, rev(preds$Q97.5) ),\n        border = 'steelblue', lty = 2) \nlines(preds$nrate, preds$Estimate, \n      col = 'blue')\n\nlegend('bottomright',\n       lty = 1:2,\n       legend = c('Estimate', 'CI'),\n       col = c('blue', 'steelblue'))\n\n\n\n\n\n\n\n\nWe get a little closer to the true value with every layer of processing that we include in this exercise. However, these layers are empirical and somewhat arbitrary. By adding a buffer zone, we are looking to remove points that have an influence from the adjacent experimental units. This can be done using the ritas algorithm within pacu."
  },
  {
    "objectID": "posts/2025-03-13_on-farm-experiment/index.html#using-pa_yield",
    "href": "posts/2025-03-13_on-farm-experiment/index.html#using-pa_yield",
    "title": "Processing data from an on-farm experiment",
    "section": "Using pa_yield",
    "text": "Using pa_yield\nThe pa_yield function has built-in capabilities to automate these processes without the need for these empirical rules. For instance, by setting the argument remove.crossed.polygons to TRUE, we remove data that could be influenced by adjacent experimental units. You can read more about the ritas algorithm here.\nSo we can visualize these steps, I will set the option steps to TRUE. This is a new addition to the package and is not yet available on CRAN. Please install the package from GitHub to be able to use this.\n\nyld3 &lt;- pa_yield(raw.yield,\n                 data.columns = c(mass = 'mass_g',\n                                  interval = 'intrvl_',\n                                  distance = 'dstnc_f',\n                                  width = 'swth_ft',\n                                  angle = 'angl_dg',\n                                  moisture = 'moistur'),\n                 grid = trial.design,\n                 steps = TRUE,\n                 algorithm = 'ritas',\n                 unit.system = 'metric',\n                 verbose = FALSE,\n                 remove.crossed.polygons = TRUE,\n                 cores = 6)\n\nyld4 &lt;- st_join(yld3$yield, trial.design, join = st_equals)\nyld4$yield_kgh &lt;- yld4$yield * 1000 ## converting from t/ha\n                                    ## to kg/ha    \nfit3 &lt;- nls(yield_kgh ~ SSlinp(nrate, a, b, xs),\n            data = yld4)\n\n## estimates\ncoef(fit3)\n\n         a          b         xs \n6997.17524   30.40087  176.09137 \n\n## variances\ndiag(vcov(fit3))\n\n           a            b           xs \n1401.4136937    0.1494841    3.1692490 \n\n## Visualizing the data\n\npreds &lt;- data.frame(nrate = 0:300)\npreds &lt;- cbind(preds,  predict_nls(fit3,\n                                   interval = 'conf',\n                                   newdata = preds))\n\nplot(yld4$nrate, \n     yld4$yield_kgh, \n     xlab = 'Nitrogen rate (kg/ha)',\n     ylab = 'Yield (kg/ha)')\npolygon(x = c(preds$nrate, rev(preds$nrate)),\n        y = c(preds$Q2.5, rev(preds$Q97.5) ),\n        border = 'steelblue', lty = 2) \nlines(preds$nrate, preds$Estimate, \n      col = 'blue')\n\n\nlegend('bottomright',\n       lty = 1:2,\n       legend = c('Estimate', 'CI'),\n       col = c('blue', 'steelblue'))\n\n\n\n\n\n\n\n\nWe can see that the pa_yield() function was able to produce more precise estimates of the model parameters. In part, this is due to the ritas algorithm recreating the destructive harvest process and being able to track which harvest polygons have crossed between different experimental units. We can see that in the plot below that demonstrates which polygons were removed:\n\nwith(yld3$steps, {\n  plot(adjusted.polygons, border = 'red')\n  plot(cleaned.polygons, add = TRUE)\n  plot(grid, add = TRUE, border = 'blue')\n  legend('topleft',\n         fill = rep(NA, 3),\n         border = c('black', 'red', 'blue'),\n         legend = c('harvest polygons', \n                    'removed polygons',\n                    'experimental units'))\n})"
  },
  {
    "objectID": "posts/2025-03-13_on-farm-experiment/index.html#conclusion",
    "href": "posts/2025-03-13_on-farm-experiment/index.html#conclusion",
    "title": "Processing data from an on-farm experiment",
    "section": "Conclusion",
    "text": "Conclusion\nIn this exercise, we have seen how different decisions about data processing can affect the estimates we get from the yield monitor data. We have also seen how we can use the pa_yield() function to process the yield data coming from agronomic trials. The differences in the precision of the estimates are really important when we want to estimate confidence intervals or investigate whether a certain covariate has an effect. For instance, let’s say that, in addition to nitrogen rates, we also had nitrogen source. More precise estimates would increase the power of our analysis to find differences."
  },
  {
    "objectID": "publications.html",
    "href": "publications.html",
    "title": "Publications",
    "section": "",
    "text": "Software\n\n\npacu: Precision Agriculture Computational Utilities, 2024  go to repository\n\n\nSoyStage - Online decision support tool for the Midsouthern U.S., 2019.  go to website\n\n\n\n\nPeer-reviewed publications\n\n\nPessotto, M.V., Roberts, T.L., dos Santos, C.L., Hoegenauer, K., Bertucci, M., Ross, J., & Savin, M. (2025). Use of growing degree days to predict aboveground biomass and total nitrogen accumulation of winter cover crops. Agrosys- tems, Geosciences & Environment, 8, 4.  go to article\n\n\ndos Santos, Caio L. & Miguez, F. E. (2025). A comparative study of yield monitor data processing methods for on-farm agronomic trials. Agronomy Journal, 117, e70168.  go to article\n\n\nCarvalho-Moore, P., Norsworthy, J., Porri, A., dos Santos, C.L., Barber, T., Sudhakar, S., Meiners, I., & Lerchl, J. (2025). Is glufosinate resitance in Palmer Amaranth spreading in Mississippi County, Arkansas? Weed Science, 73, e62.  go to article\n\n\ndos santos, C.L., & Miguez, F.E. (2024). PACU: Precision agriculture computational utilities. SoftwareX, 28, 101971.  go to article\n\n\nPessotto, M. V., Roberts, T.L., Bertucci, M., dos Santos, C., Ross, J., and Savin, M. (2023). Determining cardinal temperatures for eight cover crop species. Agrosystems, Geosciences & Environment, 6, e20393.  go to article\n\n\ndos Santos, C. L., Miguez, F. E., King, K. A., Ruiz, A., Sciarresi, C., Baum, M. E., Danalatos, G. J. N., Stellman, M., Wiley, E., Pico, L.O., Thies, A., Puntel, L. A., Topp, C. N., Trifunovic, S., Eudy, D., Mensah, C., Edwards, J. W., Schnable, P. S., Lamkey, K. R., … , and Archontoulis, S. V. (2023). Accelerated leaf appearance and flowering in maize after four decades of commercial breeding. Crop Science, 1–13.  go to article\n\n\nRuiz, A., Trifunovic, S., Eudy, D.M., Sciarresi, S. C., Baum, M., Danalatos, G.J.N., Elli, E.F., Kalogeropoulos,G., King, K., dos Santos, C.L., Thies, A., Pico, L.O., Castellano, M.J., Schnable, P.K., Topp, C., Graham, M., Lamkey, K.R., Vyn, T.J., and Archontoulis, S.V. (2023). Harvest Index has increased over the last 50 years of maize breeding. Field Crops Research, 300, 10900.  go to article\n\n\ndos Santos, C.L.; Abendroth, L.J.; Coulter, J.A.; Nafziger, E.D.; Suyker, A.; Yu, J.; Schnable, P.S.; Archontoulis, S.V. (2022). Maize Leaf Appearance Rates: A Synthesis From the United States Corn Belt. Frontiers in Plant Science, 13.  go to article\n\n\n\ndos Santos C.L., T.L. Roberts, and L.C. Purcell. (2021). Leaf Nitrogen Sufficiency Level Guidelines for Midseason Fertilization in Corn. Agronomy Journal, 113, 1974-1980.  go to article\n\n\ndos Santos, C.L., T.L. Roberts, L.C. Purcell. (2020). Canopy greenness as a midseason nitrogen management tool in corn production. Agronomy Journal. 112, 5279-5287.  go to article\n\n\ndos Santos, C.L., M. Salmerόn, and L.C. Purcell. (2019). Soybean phenology prediction tool for the Midsouth. Agricultural and Environmental Lettters, 4, 190036.  go to article\n\n\ndos Santos, C.L., A.F. De Borja Reis, P. Mazzafera, J.L. Favarin. (2018). Determination of the water potential threshold at which rice growth is impacted. Plants 7, 48.  go to article"
  },
  {
    "objectID": "posts/2025-07-19_as-applied/index.html",
    "href": "posts/2025-07-19_as-applied/index.html",
    "title": "Processing as-applied and as-planted data in on-farm trials",
    "section": "",
    "text": "In the context of on-farm research, we often apply treatments using variable rate applicators to investigate how these different treatments will affect crop yield. An important piece of extracting treatment effects is to process the “as-applied” or “as-planted” data, in the cases of fertilizer and planting data, respectively.\nWe have just released a new function called pa_trial, to process these data. The function can help us process as-applied data and aggregate these data at the level of experimental unit, so we can model the yield response to treatment.\nThe pa_trial function is still experimental, and we are still testing it and making sure that the function arguments are intuitive for general use. These might change in the development version but, when we release a final version to CRAN, these will be set. If you use the package and find any issues with the pa_trial function, please report it to us so we can fix it."
  },
  {
    "objectID": "posts/2025-07-19_as-applied/index.html#introduction",
    "href": "posts/2025-07-19_as-applied/index.html#introduction",
    "title": "Processing as-applied and as-planted data in on-farm trials",
    "section": "",
    "text": "In the context of on-farm research, we often apply treatments using variable rate applicators to investigate how these different treatments will affect crop yield. An important piece of extracting treatment effects is to process the “as-applied” or “as-planted” data, in the cases of fertilizer and planting data, respectively.\nWe have just released a new function called pa_trial, to process these data. The function can help us process as-applied data and aggregate these data at the level of experimental unit, so we can model the yield response to treatment.\nThe pa_trial function is still experimental, and we are still testing it and making sure that the function arguments are intuitive for general use. These might change in the development version but, when we release a final version to CRAN, these will be set. If you use the package and find any issues with the pa_trial function, please report it to us so we can fix it."
  },
  {
    "objectID": "posts/2025-07-19_as-applied/index.html#installing-the-necessary-packages",
    "href": "posts/2025-07-19_as-applied/index.html#installing-the-necessary-packages",
    "title": "Processing as-applied and as-planted data in on-farm trials",
    "section": "Installing the necessary packages",
    "text": "Installing the necessary packages\nThe pa_trial function can be found in the development version of pacu on GitHub. We will use two libraries for this example, pacu and sf. In case you do not have them installed, you can install them using the following code chunk\n\ninstall.packages('sf')\n\n## we will install pacu from GitHub because of some newly added features\n## version 0.1.70\nremotes::install_github('cldossantos/pacu')\n\nLet’s load the libraries:\n\nlibrary(sf)\nlibrary(pacu)"
  },
  {
    "objectID": "posts/2025-07-19_as-applied/index.html#reading-the-data-in",
    "href": "posts/2025-07-19_as-applied/index.html#reading-the-data-in",
    "title": "Processing as-applied and as-planted data in on-farm trials",
    "section": "Reading the data in",
    "text": "Reading the data in\nFor this example, we will use three simulated data sets that were built for this example. These data simulate a maize on-farm experiment that had both nitrogen and seeding rate treatments. In this particular case, I simulated an existing relationship between yield and nitrogen, but no relationship between yield and seeding rate. Let’s see if we can recover these relationships!\nHere, all the raw data that we will use in this example:\n\nTrial design: This data set contains the geometries and prescriptions for nitrogen and seeding rates\nAs-planted: This data set comes from the planter’s monitor and shows what was the planted population at each data point\nAs-applied nitrogen: This data set comes from the variable rate applicator and contains the applied nitrogen rate at each data point\nRaw yield: Data from the yield monitor showing the yield registered at each data point\n\n\ntrial.design &lt;- st_read('./raw-data/trial-design.shp')\nplanted.seed &lt;- st_read('./raw-data/as-planted.shp')\napplied.n &lt;- st_read('./raw-data/as-applied-nitrogen.shp')\nraw.yield &lt;- st_read('./raw-data/synthetic-raw-yield.shp')"
  },
  {
    "objectID": "posts/2025-07-19_as-applied/index.html#taking-a-first-look-at-the-data",
    "href": "posts/2025-07-19_as-applied/index.html#taking-a-first-look-at-the-data",
    "title": "Processing as-applied and as-planted data in on-farm trials",
    "section": "Taking a first look at the data",
    "text": "Taking a first look at the data\n\nAs-planted\nLet’s take a look at the planter’s data, which shows the seeding rate at each point. To facilitate visualization, let’s include the trial design grid as well.\n\nplot(planted.seed['plant_pop_'], \n     main = 'Seeding density (k seeds/ac)',\n     cex = 0.5,\n     pch = 16,\n     reset = FALSE)\nplot(st_geometry(trial.design), add = TRUE)\n\n\n\n\n\n\n\n\n\n\nAs-applied nitrogen\nLet’s take a look at the variable rate applicator’s data. This data shows how much urea was applied at each data point.\n\nplot(applied.n['appliedrt'], \n     main = 'Applied Urea (lb/ac)',\n     cex = 0.5,\n     pch = 16,\n     reset = FALSE)\nplot(st_geometry(trial.design), add = TRUE)\n\n\n\n\n\n\n\n\n\n\nRaw yield\nLet’s take a look at the yield monitor’s data. This will tell us what was the yield at each data point.\n\nplot(raw.yield['Yield'], \n     main = 'Maize yield (bu/ac)',\n     cex = 0.3,\n     pch = 16,\n     reset = FALSE)\n\nplot(st_geometry(trial.design), add = TRUE)"
  },
  {
    "objectID": "posts/2025-07-19_as-applied/index.html#bringing-these-datasets-together",
    "href": "posts/2025-07-19_as-applied/index.html#bringing-these-datasets-together",
    "title": "Processing as-applied and as-planted data in on-farm trials",
    "section": "Bringing these datasets together",
    "text": "Bringing these datasets together\nNow that we have seen each data set separately, we need to bring these data sets together so we can investigate the relationship between our imposed treatments and yield. To do that, we will process both the variable rate and the planter data using pa_trial, and then we will process the yield monitor data using pa_yield. For all the next steps, we will use the ritas algorithm. You can find more information about how we implemented this algorithm in the original publication of pacu.\nBefore we process these data, I will remove the big bordering area around our experimental plots just to reduce the amount of data we will process. This can be done before or after processing the data but doing it before, saves you some processing time.\n\ntrial.design &lt;- subset(trial.design, type == 'Trial')\nplot(st_geometry(trial.design))\n\n\n\n\n\n\n\n\n\nAs-planted\nHere, we will use pa_trial to process the as-planted data. The function requires the user to specify which column in the input data set corresponds to the “trial” column. In this case, the seeding density is in a column called plant_pop_. I am also specifying that the units of the planter’s width are “ft”. I use a conversion factor of 2.47 to convert from acre to hectare as well.\n\nprocessed.seed &lt;- pa_trial(planted.seed,\n                           data.columns = c(trial = 'plant_pop_'),\n                           data.units = c(trial = 'k seeds/ac',\n                                          width = 'ft'),\n                           grid = trial.design,\n                           algorithm = 'ritas',\n                           var.label = 'seeds',\n                           conversion.factor = 2.47,\n                           out.units = 'k seeds/ha',\n                           verbose = FALSE, ## suppressing progress bar\n                           cores = 6) ## for speed\n\nGuessing units of angle to be degreeN\n\n\nGuessing units of distance to be ft\n\n\nUsing pa_trial, we aggregated the seed data at the level of experimental unit, as can be seen below.\n\npa_plot(processed.seed)\n\n\n\n\n\n\n\n\n\n\nAs-applied nitrogen\nHere, we will follow the same process that we used to process the seeding density data set. The one difference is that the conversion factors change. Since the nitrogen data is as urea, we will use a conversion factor of 0.45 to reflect the nitrogen content in the urea, and then 1.12 to convert from \\(lb.ac^{-1}\\) to \\(kg.ha^{-1}\\).\n\nprocessed.nitrogen &lt;- pa_trial(applied.n,\n                               data.columns = c(trial = 'appliedrt'),\n                               data.units = c(trial = 'lb Urea/ac',\n                                              width = 'ft'),\n                               grid = trial.design,\n                               algorithm = 'ritas',\n                               var.label = 'nitrogen',\n                               conversion.factor = 0.45 * 1.12,\n                               out.units = 'kg N/ha',\n                               verbose = 0,\n                               cores = 6)\n\nGuessing units of angle to be degreeN\n\n\nGuessing units of distance to be ft\n\n\nUsing pa_trial, we aggregated the nitrogen data at the level of experimental unit, as can be seen below.\n\npa_plot(processed.nitrogen)\n\n\n\n\n\n\n\n\n\n\nMerging the treatment datasets\nNow that both treatment data sets are processed, we can combine them into a single object.\n\ntreatments &lt;- merge(processed.nitrogen, processed.seed)\nprint(treatments)\n\nVariables in trial object:\n------------------------------------------\nVariable  Algorithm  Smoothing  Units      \n---       ---        ---        ---        \nnitrogen  ritas      none       kg N/ha    \nseeds     ritas      none       k seeds/ha \n------------------------------------------\n\nVariable summary:\n------------------------\n         nitrogen  seeds \n---      ---       ---   \nMin.     59.315087 71.610\n1st Qt.  83.723312 80.764\nMedian   109.88047 86.883\n3rd Qt.  132.25985 93.063\nMax.     160.26292 101.50\nMean     108.21485 87.039\nNAs      0         0     \n------------------------\n\npa_plot(treatments)\n\n\n\n\n\n\n\n\n\n\nYield monitor data\nAfter processing all the treatment data, we can pass the treatments object to the “grid” argument and the resulting object will contain all the information regarding treatments and yield. The pa_yield function will try to guess the units of the variables it uses to process the yield monitor data. Pay attention to these and, in case they are not correct, you can override the guess by passing the argument “data.units”.\n\nprocessed.yield &lt;- pa_yield(raw.yield,\n                            grid = treatments,\n                            algorithm = 'ritas',\n                            formula = z ~ fid,\n                            smooth.method = 'krige',\n                            unit.system = 'metric',\n                            remove.crossed.polygons = TRUE,\n                            steps = TRUE,\n                            verbose = FALSE,\n                            cores = 6)\n\nGuessing units of interval to be s\n\n\nGuessing units of moisture to be %\n\n\nGuessing units of flow to be lb/s\n\n\nGuessing units of angle to be degreeN\n\n\nGuessing units of width to be ft\n\n\nGuessing units of distance to be ft\n\n\nWe can see the processed yield data below\n\npa_plot(processed.yield)"
  },
  {
    "objectID": "posts/2025-07-19_as-applied/index.html#looking-at-the-relationships-between-yield-and-the-treatment-variables",
    "href": "posts/2025-07-19_as-applied/index.html#looking-at-the-relationships-between-yield-and-the-treatment-variables",
    "title": "Processing as-applied and as-planted data in on-farm trials",
    "section": "Looking at the relationships between yield and the treatment variables",
    "text": "Looking at the relationships between yield and the treatment variables\nBelow, we can examine the relationship between yield and both treatment variables: nitrogen and seeding rate. The next steps are really independent from pacu. One can choose to model these data using whatever modeling framework they are most comfortable with, or judge the most adequate. Since the idea of this post is to demonstrate how pacu can aid in analyzing on-farm experimental data, I will stop here.\n\nwith(processed.yield$yield, \n     plot(nitrogen, \n          yield, \n          col = 4,\n          xlab = 'Nitrogen rate (kg N/ha)',\n          ylab = 'Yield (t/ha)'))\n\n\n\n\n\n\n\nwith(processed.yield$yield, \n     plot(seeds, \n          yield, \n          col = 4,\n          xlab = 'Seeding rate (k seeds/ha)',\n          ylab = 'Yield (t/ha)'))"
  },
  {
    "objectID": "posts/2025-07-19_as-applied/index.html#conclusion",
    "href": "posts/2025-07-19_as-applied/index.html#conclusion",
    "title": "Processing as-applied and as-planted data in on-farm trials",
    "section": "Conclusion",
    "text": "Conclusion\nIn this post we saw how the new pa_trial function can be used to process as-applied data from variable rate technologies in on-farm trials. We showed how to aggregate planting and nitrogen application data to the level of the experimental unit and combine them with yield monitor data for further analysis.\nBy doing so, we can move beyond simply knowing what was intended in an experimental design to understanding what was actually applied in the field—an important distinction when modeling treatment effects in precision agriculture.\nIf you have to process some of these data, you should definitely give pacu a try and let me know how it goes! We’ll be back with more updates on pacu soon! Stay tuned!"
  },
  {
    "objectID": "posts/2025-12-03_experimental-design/index.html",
    "href": "posts/2025-12-03_experimental-design/index.html",
    "title": "Experimental Design",
    "section": "",
    "text": "Recently, I was talking to some friends about experimental design (exciting topic, right?!) and we reached a topic that I had not given much thought to before. When we are planning an experiment, how many controls should we include? I guess the answer to that question is often the usual agronomist answer: it depends.\nIn many agronomic experiments, this question really hinges on how we conceptualize the “control.” If “control” is simply the 0 rate of a fertilizer, then it is a level of the rate factor, not a separate source. But if we treat “control” as its own fertilizer source, we have implicitly changed the structure of the experiment.\nIn this post, I will explore some of my ideas on the topic. I do not claim that they are universally valid, but I will explain my train of thought so you can judge whether these ideas apply to your case. Let’s dive into it."
  },
  {
    "objectID": "posts/2025-12-03_experimental-design/index.html#introduction",
    "href": "posts/2025-12-03_experimental-design/index.html#introduction",
    "title": "Experimental Design",
    "section": "",
    "text": "Recently, I was talking to some friends about experimental design (exciting topic, right?!) and we reached a topic that I had not given much thought to before. When we are planning an experiment, how many controls should we include? I guess the answer to that question is often the usual agronomist answer: it depends.\nIn many agronomic experiments, this question really hinges on how we conceptualize the “control.” If “control” is simply the 0 rate of a fertilizer, then it is a level of the rate factor, not a separate source. But if we treat “control” as its own fertilizer source, we have implicitly changed the structure of the experiment.\nIn this post, I will explore some of my ideas on the topic. I do not claim that they are universally valid, but I will explain my train of thought so you can judge whether these ideas apply to your case. Let’s dive into it."
  },
  {
    "objectID": "posts/2025-12-03_experimental-design/index.html#how-did-this-discussion-start",
    "href": "posts/2025-12-03_experimental-design/index.html#how-did-this-discussion-start",
    "title": "Experimental Design",
    "section": "How did this discussion start?",
    "text": "How did this discussion start?\nWe started talking about this when thinking about how to analyze an experiment that had already been conducted in a controlled environment. The experiment was relatively simple, and the researchers wanted to evaluate the effect of fertilizer source (e.g., synthetic fertilizer, manure, digestate) and rate (i.e., how much fertilizer was applied) on nitrogen oxide emissions. In addition, they had four replications of the treatments.\nSo, this experiment had two factors: rate and source. If this experiment were planned to follow a factorial design, we would have had all combinations of all levels of both factors. The nitrogen rates ranged from 0 to 300 kg/ha, in 60 kg/ha intervals, and there were 3 sources of nitrogen. The experimental design should look something like this:\n\nrates &lt;- seq(0, 300, 60)\nsources &lt;- c('synthetic', 'manure', 'digestate')\nreplications &lt;- 1:4\nexperimental.structure &lt;- expand.grid(rate = rates,\n                                      source = sources,\n                                      rep = replications,\n                                      stringsAsFactors = FALSE)\n\nThe number of experimental units per combination of factors would be four, resulting in 72 experimental units.\n\nwith(experimental.structure,addmargins(table(source, rate)))\n\n           rate\nsource       0 60 120 180 240 300 Sum\n  digestate  4  4   4   4   4   4  24\n  manure     4  4   4   4   4   4  24\n  synthetic  4  4   4   4   4   4  24\n  Sum       12 12  12  12  12  12  72\n\n\nHowever, that’s not how the experiment was planned. Since the rate “0” means that no nitrogen source was applied, the researchers decided to include a single set of 0-nitrogen experimental units to serve as their control. By doing this, they eliminated 8 experimental units from the design, potentially saving time and resources but, in my view, creating some problems for the analysis. This is not the first time I have seen this—this strategy is common in agronomic experiments. As someone who has collected a lot of data in the field, I understand the urge to save resources. However, let’s take a look at what this means for the analysis.\n\nexperimental.structure2 &lt;- experimental.structure\nexperimental.structure2 &lt;- subset(experimental.structure2,\n                                  !(rate == 0 & source %in% c('manure', 'digestate')))\nexperimental.structure2$source[experimental.structure2$rate == 0] &lt;- 'control'\nwith(experimental.structure2,addmargins(table(source, rate)))\n\n           rate\nsource       0 60 120 180 240 300 Sum\n  control    4  0   0   0   0   0   4\n  digestate  0  4   4   4   4   4  20\n  manure     0  4   4   4   4   4  20\n  synthetic  0  4   4   4   4   4  20\n  Sum        4 12  12  12  12  12  64\n\n\nThe table above brings to light a problem that this experimental design has in the analysis stage. Should the “control” experimental units be in their own “source” level? Or should they be assigned to one of the pre-existing levels of source? I think we can understand the implications of this experimental design a little bit better with a more concrete example."
  },
  {
    "objectID": "posts/2025-12-03_experimental-design/index.html#lets-build-an-example",
    "href": "posts/2025-12-03_experimental-design/index.html#lets-build-an-example",
    "title": "Experimental Design",
    "section": "Let’s build an example",
    "text": "Let’s build an example\nLet us build a conceptual example in which both manure and digestate present similar responses to nitrogen rate in terms of nitrogen emissions, and synthetic fertilizer presents a greater potential for emissions.\nThis conceptual response to nitrogen rate can be seen in this function, in which the rate of emission increases when the source is “synthetic.” For this exercise, we will pretend that this is the true relationship that we are trying to uncover by running this experiment.\n\nconceptual.response &lt;- function(x, a, b, src){\n  if(src == 'synthetic')\n    b &lt;- b + 0.05\n  resp &lt;- a + b * x\n  resp\n}\n\nconceptual.response &lt;- Vectorize(conceptual.response,\n                                 c('x', 'src'))\nxvec &lt;- 0:300\nplot(xvec,\n     conceptual.response(xvec, 6, 0.1, 'not synthetic'),\n     xlab = 'N rate', \n     ylab = 'Emissions', \n     type = 'l')\nlines(xvec,\n     conceptual.response(xvec, 6, 0.1, 'synthetic'),\n     col = 'red')\nlegend('topleft',\n       lty = 1,\n       col = c('black', 'red'),\n       legend = c('Not synthetic', 'synthetic'))\n\n\n\n\n\n\n\n\nHere I am using a simple linear relationship to keep the example transparent. This is not meant to imply emissions must be linear in real systems, just that linear models clearly reveal what happens when treatment combinations are missing. The slopes in the simulated example (0.01 for non-synthetic and +0.05 additional slope for synthetic) are chosen for numerical clarity rather than biological realism."
  },
  {
    "objectID": "posts/2025-12-03_experimental-design/index.html#factorial-design",
    "href": "posts/2025-12-03_experimental-design/index.html#factorial-design",
    "title": "Experimental Design",
    "section": "Factorial design",
    "text": "Factorial design\nLet’s see how well we can uncover this relationship when we use a full factorial design.\n\nnoise &lt;- rnorm(nrow(experimental.structure),\n               mean = 0,\n               sd = 2)\nexperimental.structure$emissions &lt;- with(experimental.structure,\n                                         conceptual.response(rate,\n                                                             6, \n                                                             0.01,\n                                                             source)) + noise\n\n\nplot(experimental.structure$rate,\n     experimental.structure$emissions,\n     pch = 16,\n     col = hcl.colors(3, 'Set 2')[as.factor(experimental.structure$source)],\n     xlab = 'N rate',\n     ylab = 'Emissions')\n\nlegend('topleft',\n       pch = 16,\n       col =  hcl.colors(3, 'Set2'),\n       legend = levels(as.factor(experimental.structure$source)))\n\n\n\n\n\n\n\n\nHere, we have all levels represented and the mock data can be used to model the relationship between our response variable and the factors rate and source. This model was able to estimate all the effects that we were interested in. Now, let’s see what happens when we include only one control.\n\nfit1 &lt;- lm(emissions ~ rate + source + rate:source,\n           data = experimental.structure)\nsummary(fit1)\n\n\nCall:\nlm(formula = emissions ~ rate + source + rate:source, data = experimental.structure)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-5.7436 -1.3800  0.1929  1.8327  4.8022 \n\nCoefficients:\n                      Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)           5.788803   0.807403   7.170 8.08e-10 ***\nrate                  0.011973   0.004445   2.694  0.00895 ** \nsourcemanure          1.593838   1.141841   1.396  0.16744    \nsourcesynthetic      -0.099181   1.141841  -0.087  0.93105    \nrate:sourcemanure    -0.004823   0.006286  -0.767  0.44563    \nrate:sourcesynthetic  0.047252   0.006286   7.518 1.93e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.231 on 66 degrees of freedom\nMultiple R-squared:  0.8322,    Adjusted R-squared:  0.8195 \nF-statistic: 65.45 on 5 and 66 DF,  p-value: &lt; 2.2e-16\n\n\nThis model correctly estimates all main effects and interactions because the design contains every treatment combination. Importantly, the model is assuming linearity with respect to rate, but that choice is independent from the factorial structure. Even nonlinear models would require all treatment combinations to be present to identify source-specific responses."
  },
  {
    "objectID": "posts/2025-12-03_experimental-design/index.html#including-only-one-control",
    "href": "posts/2025-12-03_experimental-design/index.html#including-only-one-control",
    "title": "Experimental Design",
    "section": "Including only one control",
    "text": "Including only one control\n\nnoise &lt;- rnorm(nrow(experimental.structure2),\n               mean = 0,\n               sd = 2)\nexperimental.structure2$emissions &lt;- with(experimental.structure2,\n                                         conceptual.response(rate,\n                                                             6, \n                                                             0.01,\n                                                             source)) + noise\n\nHere, we can see a first source of uncertainty/awkwardness. We need to decide what to do with the “control” plots. Do we leave them by themselves in the “control” category? Should we assign the control to one of the other fertilizer sources? Let’s proceed by leaving the “control” experimental units in their own category.\n\nControl experimental units in their own category\nLet’s visualize these data to understand this source of uncertainty better. We have four observations sitting at the 0-nitrogen rate and assigned to the control treatment. Somehow, these observations are supposed to help us estimate the relationship between nitrogen emissions and nitrogen rate for the other three “source” treatments.\nNext, we can proceed to fitting the same model as before and see if we can estimate all model parameters.\n\nplot(experimental.structure2$rate,\n     experimental.structure2$emissions,\n     pch = 16,\n     col = hcl.colors(4, 'Set 2')[as.factor(experimental.structure2$source)],\n     xlab = 'N rate',\n     ylab = 'Emissions')\n\nlegend('topleft',\n       pch = 16,\n       col =  hcl.colors(4, 'Set2'),\n       legend = levels(as.factor(experimental.structure2$source)))\n\n\n\n\n\n\n\n\n\nfit2.1 &lt;- lm(emissions ~ rate + source + rate:source,\n           data = experimental.structure2)\n\nsummary(fit2.1)\n\n\nCall:\nlm(formula = emissions ~ rate + source + rate:source, data = experimental.structure2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.9262 -1.0534 -0.1088  0.9185  4.6713 \n\nCoefficients: (1 not defined because of singularities)\n                      Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)           5.279113   0.963527   5.479 1.01e-06 ***\nrate                  0.057687   0.005078  11.360 2.86e-16 ***\nsourcedigestate      -0.080129   1.396284  -0.057    0.954    \nsourcemanure          1.594337   1.396284   1.142    0.258    \nsourcesynthetic       1.043917   1.396284   0.748    0.458    \nrate:sourcedigestate -0.042891   0.007182  -5.972 1.60e-07 ***\nrate:sourcemanure    -0.048135   0.007182  -6.702 9.95e-09 ***\nrate:sourcesynthetic        NA         NA      NA       NA    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.927 on 57 degrees of freedom\nMultiple R-squared:  0.884, Adjusted R-squared:  0.8718 \nF-statistic: 72.39 on 6 and 57 DF,  p-value: &lt; 2.2e-16\n\n\nOne of the first things we can see is that estimating the interaction terms can be problematic. We are missing some levels, which can be seen as zero counts in this table. This prevents us from estimating the interaction terms.\n\nwith(experimental.structure2, addmargins(table(source, rate)))\n\n           rate\nsource       0 60 120 180 240 300 Sum\n  control    4  0   0   0   0   0   4\n  digestate  0  4   4   4   4   4  20\n  manure     0  4   4   4   4   4  20\n  synthetic  0  4   4   4   4   4  20\n  Sum        4 12  12  12  12  12  64\n\n\n\n\nReplicating the control data\nOne of the ways I have seen researchers deal with this problem in the analysis is by replicating the control data to the other experimental units. I think the idea is that the control should present about the same variability regardless of treatment combination. However, I believe this is a major violation of the model assumptions. Namely, this violates the assumption of independence of the residuals. Are there residuals more dependent among themselves than those from copied data? :)\nI will show here what I mean by this, but I am not recommending this to anyone.\n\nexperimental.structure3 &lt;- experimental.structure2\ncontrol.indices &lt;- which(experimental.structure3$source == 'control')\nexperimental.structure3 &lt;- rbind(experimental.structure3,\n                                 experimental.structure3[rep(control.indices, 2), ])\n\ncontrol.indices &lt;- which(experimental.structure3$source == 'control')\nexperimental.structure3$source[control.indices] &lt;- rep(unique(experimental.structure$source), rep(4, 3))\n\nThis data set now contains a structure similar to the first data set, but models fit using these data violate an important assumption of regression models. Additionally, this approach artificially inflates the number of observations in the data set. In turn, this reduces standard errors and increases the chances of finding differences when there are none.\n\nwith(experimental.structure3, addmargins(table(source, rate)))\n\n           rate\nsource       0 60 120 180 240 300 Sum\n  digestate  4  4   4   4   4   4  24\n  manure     4  4   4   4   4   4  24\n  synthetic  4  4   4   4   4   4  24\n  Sum       12 12  12  12  12  12  72"
  },
  {
    "objectID": "posts/2025-12-03_experimental-design/index.html#my-two-cents",
    "href": "posts/2025-12-03_experimental-design/index.html#my-two-cents",
    "title": "Experimental Design",
    "section": "My two cents :)",
    "text": "My two cents :)\nI decided to write this text out of a conversation I had with a friend to show how I think about this. I believe there might be people out there with alternative points of view, and I would love to hear more about it. For what it is worth, I believe that experimental design is an important and often overlooked step in the scientific process. I can only imagine how frustrating it would be to run a trial and then not be able to run the analysis correctly.\nI think the main message here is that statistical identifiability is not something we can recover after the fact. Once certain treatment combinations are missing from the experimental design, we might not be able to answer the scientific questions that we set out to investigate."
  },
  {
    "objectID": "posts/2025-02-11_yield-map/index.html",
    "href": "posts/2025-02-11_yield-map/index.html",
    "title": "Working with yield monitor data",
    "section": "",
    "text": "Working with yield monitor data can be quite challenging. The data can have several sources of error and accounting for all of them can be hard. For instance, there are instances in which the combine travels through an area that has been previously harvested, artificially recording low yielding points. There can also uncertainties associated with the combine travelling fast or slow, changing the effective harvested area from one observation to the next. There are many other instances in which we can end up with inadequate data when working with yield monitor data but this blog post cannot list all of them.\nI wanted to give you an example of how we can use pacu to address some of these challenges. For that, we will use a data set contained in the agridat package."
  },
  {
    "objectID": "posts/2025-02-11_yield-map/index.html#introduction",
    "href": "posts/2025-02-11_yield-map/index.html#introduction",
    "title": "Working with yield monitor data",
    "section": "",
    "text": "Working with yield monitor data can be quite challenging. The data can have several sources of error and accounting for all of them can be hard. For instance, there are instances in which the combine travels through an area that has been previously harvested, artificially recording low yielding points. There can also uncertainties associated with the combine travelling fast or slow, changing the effective harvested area from one observation to the next. There are many other instances in which we can end up with inadequate data when working with yield monitor data but this blog post cannot list all of them.\nI wanted to give you an example of how we can use pacu to address some of these challenges. For that, we will use a data set contained in the agridat package."
  },
  {
    "objectID": "posts/2025-02-11_yield-map/index.html#installing-and-loading-the-necessary-packages",
    "href": "posts/2025-02-11_yield-map/index.html#installing-and-loading-the-necessary-packages",
    "title": "Working with yield monitor data",
    "section": "Installing and loading the necessary packages",
    "text": "Installing and loading the necessary packages\nIf you have not done so, you can install the agridat package using the following line of code:\n\ninstall.packages('agridat')\n\nTo install pacu, you can either install the CRAN version:\n\ninstall.packages('pacu')\n\nOr, you can install the development version from GitHub using the remotes package:\n\nremotes::install_github('cldossantos/pacu')\n\nNow that we have installed the necessary packages we can load them and continue with our analysis.\n\nlibrary(pacu)\nlibrary(sf)\nlibrary(agridat)"
  },
  {
    "objectID": "posts/2025-02-11_yield-map/index.html#working-with-the-data",
    "href": "posts/2025-02-11_yield-map/index.html#working-with-the-data",
    "title": "Working with yield monitor data",
    "section": "Working with the data",
    "text": "Working with the data\nThe agridat package contains a data set of yield observations collected from a corn field in Minnesota, the data set name is gartner.corn.\nHere, we load the data set and look at the first rows:\n\ndata(\"gartner.corn\")\nhead(gartner.corn)\n\n       long      lat  mass time seconds dist moist    elev\n1 -93.97842 43.92726 16.54    0       3  116  18.5 1030.58\n2 -93.97842 43.92723 22.52    3       3  159  16.7 1030.58\n3 -93.97842 43.92718 27.01    6       3  169  17.2 1029.92\n4 -93.97842 43.92713 30.24    9       3  221  17.2 1029.92\n5 -93.97842 43.92708 30.95   12       3  234  17.3 1029.59\n6 -93.97842 43.92702 33.57   15       3  227  17.5 1029.59\n\n\nThere are a couple pieces of information that we need but that are currently not included in the data set. Namely, we need the combine swath and the yield. In the help page the author provided more information that can come in handy. For instance, he told us that the combine swath is 360 inches and provided a formula to calculate yield from the information in the data set. The yield will be calculated in units of bushel/acre. Let’s follow his formula:\n\ngartner.corn$swath &lt;- 360\ngartner.corn$dry.grain &lt;-  with(gartner.corn, (mass * seconds * (100-moist) / (100-15.5)) / 56) \ngartner.corn$harvested.area &lt;-  with(gartner.corn, (dist * swath) / 6272640) \ngartner.corn$yield &lt;- with(gartner.corn, dry.grain / harvested.area)"
  },
  {
    "objectID": "posts/2025-02-11_yield-map/index.html#renaming-some-of-the-variables",
    "href": "posts/2025-02-11_yield-map/index.html#renaming-some-of-the-variables",
    "title": "Working with yield monitor data",
    "section": "Renaming some of the variables",
    "text": "Renaming some of the variables\nHere, I rename some of the variables so that it is easier for me to understand what they represent.\n\nnames(gartner.corn) &lt;- c('long', 'lat', 'flow', 'time', 'interval', 'distance', 'moisture', 'elevation', 'swath', 'dry.grain', 'harvested.area', 'yield')\n\nNow that we have added the necessary columns, we can convert the data frame into a sf object. The sf library has several methods for working with spatial data and pacu is heavily built upon those. We can also plot the data to look into the spatial patterns of the variables:\n\nyield.data &lt;- st_as_sf(gartner.corn, \n                       coords = c('long', 'lat'),\n                       crs = 'epsg:4326')\nplot(yield.data)"
  },
  {
    "objectID": "posts/2025-02-11_yield-map/index.html#looking-at-the-yield-data",
    "href": "posts/2025-02-11_yield-map/index.html#looking-at-the-yield-data",
    "title": "Working with yield monitor data",
    "section": "Looking at the yield data",
    "text": "Looking at the yield data\nIf we focus on the yield (bu/ac), we can see some interesting features of this field. It seems like there is a waterway in the northern part of the field and there is an area in the mid-lower east part of the field that has lower yields.\n\nplot(yield.data['yield'], pch = 15)\n\n\n\n\n\n\n\n\nSomething that can also catch our attention is just how variable this data is. Let’s take a look at the distribution of the data as well. We can see that most of the data is between 100 and 160 bu/ac but the data ranges from 0 to 258 bu/ac.\n\nplot(density(yield.data$yield), main = '')"
  },
  {
    "objectID": "posts/2025-02-11_yield-map/index.html#considerations-about-cleaning-the-data",
    "href": "posts/2025-02-11_yield-map/index.html#considerations-about-cleaning-the-data",
    "title": "Working with yield monitor data",
    "section": "Considerations about cleaning the data",
    "text": "Considerations about cleaning the data\nThis kind of variability can be dealt with a variety of approaches. There are researchers who have proposed that we can remove anything that falls outside of 2 or 3 standard deviations from the mean. This is an empirical rule based on the assumption that the yield data follows a normal distribution. These thresholds represent \\(\\approx95.0\\%\\) and \\(\\approx99.7\\%\\) of the probability mass function of a normal distribution, respectively. However, this is based on two assumptions that are violated from the beginning:\n\nIndependent samples: these are correlated in space\nNormal distribution: there is no guarantee this data will follow a normal distribution. These data are bound to be greater than zero.\n\nThere is ultimately no magical formula for cleaning yield data. Some of the empirical rules can work for some data sets but not for others. pacu offers options that do not rely on these rules but I feel that this is a subject for a different post."
  },
  {
    "objectID": "posts/2025-02-11_yield-map/index.html#producing-a-yield-map",
    "href": "posts/2025-02-11_yield-map/index.html#producing-a-yield-map",
    "title": "Working with yield monitor data",
    "section": "Producing a yield map",
    "text": "Producing a yield map\nTo produce a yield map using pacu, we will use the pa_yield() function. The package offers two algorithms: simple and ritas. In this post I will not go into much detail about the algorithms. There is more information about this in this paper.\nIn this case, I will go straight to the ritas algorithm. The ritas algorithm involves several computationally intensive processes. To accelerate this process, we have enabled parallelization. The user can control this using the cores argument. Keep in mind though that parallelization has diminishing returns!\nI did not supply units in this case because the pa_yield() function attempts to guess the units and I wanted to demonstrate that functionality. However, this is based on very simple rules and the function can make a mistake. In that case, the user can override the guess by passing the argument data.units.\n\nyld &lt;- pa_yield(input = yield.data, \n                algorithm = 'ritas',\n                unit.system = 'metric',\n                moisture.adj = 15.5,\n                cores = 5,\n                verbose = FALSE)\n\nGuessing units of interval to be s\n\n\nGuessing units of moisture to be %\n\n\nGuessing units of flow to be lb/s\n\n\nGuessing units of width to be in\n\n\nGuessing units of distance to be in\n\n\nTo look at the yield map, we can use the pa_plot() function.\n\npa_plot(yld, legend.outside = TRUE)"
  },
  {
    "objectID": "posts/2025-02-11_yield-map/index.html#conclusion",
    "href": "posts/2025-02-11_yield-map/index.html#conclusion",
    "title": "Working with yield monitor data",
    "section": "Conclusion",
    "text": "Conclusion\nWe have looked into how we can build yield maps from raw yield monitor data using pacu. This is a case in which we are processing the data at the field level. In a case in which we are processing data from on-farm experiments, there are a few more considerations we need to make. This is a subject for a future post!"
  },
  {
    "objectID": "cv.html",
    "href": "cv.html",
    "title": "CV",
    "section": "",
    "text": "If you have trouble visualizing the embeded CV, please click  here to download the pdf file."
  }
]